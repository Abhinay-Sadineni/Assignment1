{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment1: Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "posts_df = pd.read_csv('./csv/Posts.csv')\n",
    "Tags = pd.read_csv('./csv/Tags.csv')\n",
    "\n",
    "answers_df = posts_df[posts_df['PostTypeId'] == 2][['Id', 'OwnerUserId', 'ParentId']]\n",
    "answers_df = answers_df.drop_duplicates(subset=['OwnerUserId','ParentId'])\n",
    "answers_df['ParentId'] = answers_df['ParentId'].astype(int)\n",
    "\n",
    "questions_df = posts_df[posts_df['PostTypeId'] == 1][['Id', 'Tags']]\n",
    "\n",
    "questions_df['Id'] = questions_df['Id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 users with the most answers:\n",
      "       OwnerUserId  AnswerCount\n",
      "3189        9113.0         2838\n",
      "19912     177980.0         2318\n",
      "557         1204.0         2042\n",
      "\n",
      "Top 3 most used tags:\n",
      "    TagName  Count\n",
      "259  design   5162\n",
      "114      c#   4931\n",
      "37     java   4929\n"
     ]
    }
   ],
   "source": [
    "answerers_table = answers_df.groupby('OwnerUserId').size().reset_index(name='AnswerCount')\n",
    "top_answerers = answerers_table.sort_values(by='AnswerCount', ascending=False).head(3)\n",
    "\n",
    "top_tags = Tags[['TagName', 'Count']].sort_values(by='Count', ascending=False).head(3)\n",
    "\n",
    "print(\"Top 3 users with the most answers:\")\n",
    "print(top_answerers)\n",
    "\n",
    "print(\"\\nTop 3 most used tags:\")\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1 : First attach corresponding tags for answers by table join with questions table using Parent Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Qualified Answerers: 1160\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(answers_df, questions_df, left_on='ParentId', right_on='Id', suffixes=('_answer', '_question'))\n",
    "\n",
    "filtered_answers_df = merged_df[['Id_answer', 'OwnerUserId', 'Tags','ParentId']] \n",
    "filtered_answers_df = filtered_answers_df.drop_duplicates(['OwnerUserId','ParentId'])\n",
    "answerer_counts = filtered_answers_df.groupby('OwnerUserId').size()\n",
    "qualified_answerers = answerer_counts[answerer_counts >= 20].index\n",
    "print(\"No. of Qualified Answerers:\", len(qualified_answerers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step2 : Filter the answers using the qualified answers ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered answerers:    Id_answer  OwnerUserId                                               Tags  \\\n",
      "0          3         11.0                           |comments|anti-patterns|   \n",
      "3         13          4.0                           |comments|anti-patterns|   \n",
      "4         20          6.0                     |productivity|time-management|   \n",
      "6         23         11.0                     |productivity|time-management|   \n",
      "8         26         17.0  |business|project-management|development-process|   \n",
      "\n",
      "   ParentId  \n",
      "0         1  \n",
      "3         1  \n",
      "4         9  \n",
      "6         9  \n",
      "8         4  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_answers = filtered_answers_df[filtered_answers_df['OwnerUserId'].isin(qualified_answerers)]\n",
    "print(\"filtered answerers:\" , filtered_answers.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step3 :  Filter tags and expand the answers tables by expanding rows for each tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualified Tags: 974\n"
     ]
    }
   ],
   "source": [
    "qualified_tags = Tags[Tags['Count'] >= 20]['Id']\n",
    "print(\"Qualified Tags:\", len(qualified_tags))\n",
    "\n",
    "tag_dict = Tags.set_index('TagName')['Id'].to_dict()\n",
    "\n",
    "tags_expanded = filtered_answers.copy()\n",
    "\n",
    "tags_expanded['Tags'] = tags_expanded['Tags'].str.split('|').apply(lambda x: x[1:-1])\n",
    "tags_expanded = tags_expanded.explode('Tags')\n",
    "tags_expanded['Tags'] = tags_expanded['Tags'].map(tag_dict)\n",
    "tags_expanded = tags_expanded[tags_expanded['Tags'].isin(qualified_tags)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step4 : Create Utility matrix from the filtered answers table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert Matrix: Tags         1     3     4     7     8     9     11    12    13    14    ...  \\\n",
      "OwnerUserId                                                              ...   \n",
      "4            13.0   NaN   6.0   6.0  61.0  55.0   8.0   3.0   NaN   NaN  ...   \n",
      "6             NaN   NaN   8.0   NaN   6.0   4.0   1.0   2.0   NaN   NaN  ...   \n",
      "11            1.0   NaN   1.0   NaN   NaN   1.0   NaN   1.0   NaN   NaN  ...   \n",
      "14            NaN   NaN   1.0   NaN   1.0   1.0   NaN   1.0   NaN   NaN  ...   \n",
      "15            1.0   NaN   2.0   1.0   4.0   4.0   1.0   1.0   NaN   NaN  ...   \n",
      "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
      "356695        NaN   NaN   NaN   NaN   NaN   1.0   NaN   NaN   NaN   NaN  ...   \n",
      "366014        NaN   NaN   NaN   NaN   NaN   NaN   1.0   NaN   NaN   NaN  ...   \n",
      "373864        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "378329        1.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "379622        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "\n",
      "Tags         4639  4646  4661  4682  4683  4687  4690  4704  4720  4750  \n",
      "OwnerUserId                                                              \n",
      "4             NaN   NaN   NaN   2.0   1.0   1.0   NaN   NaN   1.0   NaN  \n",
      "6             NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "11            NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "14            NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "15            NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "356695        NaN   NaN   1.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "366014        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "373864        NaN   NaN   2.0   NaN   NaN   NaN   1.0   NaN   NaN   NaN  \n",
      "378329        NaN   NaN   1.0   NaN   NaN   NaN   NaN   NaN   5.0   1.0  \n",
      "379622        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "\n",
      "[1160 rows x 973 columns]\n",
      "Dimensions of the Expert matrix: (1160, 973)\n"
     ]
    }
   ],
   "source": [
    "expert_matrix_df = pd.pivot_table(\n",
    "    tags_expanded, \n",
    "    index='OwnerUserId', \n",
    "    columns='Tags', \n",
    "    aggfunc='size', \n",
    "    fill_value=np.nan\n",
    ")\n",
    "\n",
    "expert_matrix_df.index = expert_matrix_df.index.astype(int)\n",
    "expert_matrix_df.columns = expert_matrix_df.columns.astype(int)\n",
    "\n",
    "all_qualified_tags = pd.Series(qualified_tags, name='Tags')\n",
    "print(\"Expert Matrix:\", expert_matrix_df)\n",
    "print(\"Dimensions of the Expert matrix:\", expert_matrix_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step5: convert to numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13. nan  6. ... nan  1. nan]\n",
      " [nan nan  8. ... nan nan nan]\n",
      " [ 1. nan  1. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 1. nan nan ... nan  5.  1.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "expert_matrix = expert_matrix_df.to_numpy()\n",
    "print(expert_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Normalize the utility matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "normalize = lambda x: np.nan if np.isnan(x) else (np.floor(x / 3) if x < 15 else 5)\n",
    "vectorized_modify_entries = np.vectorize(normalize)\n",
    "utility_matrix = vectorized_modify_entries(expert_matrix)\n",
    "utility_matrix_df = expert_matrix_df.map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4. nan  2. ... nan  0. nan]\n",
      " [nan nan  2. ... nan nan nan]\n",
      " [ 0. nan  0. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 0. nan nan ... nan  1.  0.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(utility_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility Matrix Metrics:\n",
      "Summation value of the utility matrix: 41180.0\n",
      "Highest row sum of the utility matrix: 1162.0\n",
      "Highest column sum of the utility matrix: 1403.0\n"
     ]
    }
   ],
   "source": [
    "sum_utility_matrix = np.nansum(utility_matrix)\n",
    "highest_row_sum = np.max(np.nansum(utility_matrix, axis=1))\n",
    "highest_column_sum = np.max(np.nansum(utility_matrix, axis=0))\n",
    "\n",
    "print(\"Utility Matrix Metrics:\")\n",
    "print(\"Summation value of the utility matrix:\", sum_utility_matrix)\n",
    "print(\"Highest row sum of the utility matrix:\", highest_row_sum)\n",
    "print(\"Highest column sum of the utility matrix:\", highest_column_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Create Train and Test matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test Matrix Metrics:\n",
      "Summation value of the train matrix: 40538.0\n",
      "Dimension of Test Matrix: (174, 146)\n",
      "Summation value of the test matrix: 642.0\n"
     ]
    }
   ],
   "source": [
    "num_users, num_tags = utility_matrix.shape\n",
    "user_cutoff = int(num_users * 0.85)\n",
    "tag_cutoff = int(num_tags * 0.85)\n",
    "\n",
    "train_matrix = utility_matrix.copy()\n",
    "train_df = utility_matrix_df.copy()\n",
    "\n",
    "\n",
    "train_matrix[user_cutoff:, tag_cutoff:] = np.nan\n",
    "train_df.iloc[user_cutoff: ,tag_cutoff:] = np.nan\n",
    "train_matrix_sum = np.nansum(train_matrix)\n",
    "\n",
    "test_matrix = utility_matrix[user_cutoff:, tag_cutoff:]\n",
    "test_df = utility_matrix_df.iloc[user_cutoff: , tag_cutoff:]\n",
    "sum_test_matrix = np.nansum(test_matrix)\n",
    "\n",
    "print(\"Train and Test Matrix Metrics:\")\n",
    "print(\"Summation value of the train matrix:\", train_matrix_sum)\n",
    "print(\"Dimension of Test Matrix:\",test_matrix.shape)\n",
    "print(\"Summation value of the test matrix:\", sum_test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag-Tag Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-1: Calculate Similarity matrix for Tag-Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = utility_matrix[:user_cutoff]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_centered = df.sub(df.mean(axis=1), axis=0)\n",
    "\n",
    "similarity_matrix = df_centered.corr(method='pearson')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-2: Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, item_id, ratings, similarity_matrix, N , type):\n",
    "    \n",
    "    item_similarities = np.array(similarity_matrix[item_id][:tag_cutoff])\n",
    "    user_ratings = np.array(ratings[user_id])\n",
    "    similar_items = np.argsort(item_similarities)[::-1]\n",
    "\n",
    "    start = 0\n",
    "    while start < len(similar_items):\n",
    "        if np.isnan(item_similarities[similar_items[start]]): start += 1\n",
    "        else : break\n",
    "    \n",
    "    non_nan_similar_items = similar_items[start:]\n",
    "\n",
    "    rated_items = [item for item in non_nan_similar_items \n",
    "                         if not np.isnan(ratings[user_id][item])]\n",
    "    top_n_rated_items = rated_items[:N]  \n",
    "\n",
    "    if len(top_n_rated_items) < N:\n",
    "        return np.nanmean(user_ratings)\n",
    "    \n",
    "    top_n_ratings = user_ratings[top_n_rated_items]\n",
    "    top_n_similarities = item_similarities[top_n_rated_items]\n",
    "    \n",
    "    if type == \"A\":\n",
    "     return np.sum(top_n_ratings) / N\n",
    "\n",
    "    weighted_ratings_sum = np.dot(top_n_ratings, top_n_similarities)\n",
    "\n",
    "    similarity_sum = np.sum(top_n_similarities)\n",
    "    if similarity_sum != 0 : \n",
    "        predicted_rating = weighted_ratings_sum / similarity_sum\n",
    "    else : \n",
    "        predicted_rating = 0\n",
    "    \n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-3: Predict and Calculate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Average Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Average\n",
      "Loss with k = 2 : 0.8368331915377201\n",
      "Loss with k = 3 : 0.8068537836476033\n",
      "Loss with k = 5 : 0.7667057566908195\n"
     ]
    }
   ],
   "source": [
    "print(\"Simple Average\")\n",
    "for k in {2,3,5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x in range(user_cutoff,num_users):\n",
    "        for i in range(tag_cutoff,num_tags):\n",
    "            if np.isnan(utility_matrix[x][i]): continue\n",
    "            else : \n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x,i,utility_matrix,similarity_matrix,k,\"A\")\n",
    "                if(np.isnan(ans)) : \n",
    "                    print(x,i,'Hello')\n",
    "                loss = loss + (true_value - ans)*(true_value - ans)\n",
    "                cnt += 1\n",
    "    print(\"Loss with k =\",k,\":\",np.sqrt(loss / cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weighted Average Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average\n",
      "Loss with k = 2 : 0.8368997316823331\n",
      "Loss with k = 3 : 0.8066557194070604\n",
      "Loss with k = 5 : 0.768163144485662\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted Average\")\n",
    "for k in {2,3,5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x in range(user_cutoff,num_users):\n",
    "        for i in range(tag_cutoff,num_tags):\n",
    "            if np.isnan(utility_matrix[x][i]): continue\n",
    "            else : \n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x,i,utility_matrix,similarity_matrix,k,\"B\")\n",
    "                if(np.isnan(ans)) : \n",
    "                    print(x,i,'Hello')\n",
    "                loss = loss + (true_value - ans)*(true_value - ans)\n",
    "                cnt += 1\n",
    "    print(\"Loss with k =\",k,\":\",np.sqrt(loss / cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-User Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1160, 827)\n",
      "(1160, 1160)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = utility_matrix[:, :tag_cutoff]\n",
    "print(data.shape)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df_centered = df.sub(df.mean(axis=0), axis=1)\n",
    "\n",
    "similarity_matrix = df_centered.T.corr(method='pearson')\n",
    "print(similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, item_id, ratings, similarity_matrix, N, type, user_cutoff):\n",
    "    # Get the similarity scores for the target user with all other users within the user_cutoff\n",
    "    user_similarities = np.array(similarity_matrix[user_id][:user_cutoff])\n",
    "\n",
    "    # Get the ratings for the item by all users within the user_cutoff\n",
    "    item_ratings = np.array(ratings[:,item_id])\n",
    "\n",
    "    # Sort the similarities and take the top N most similar users (excluding the target user itself)\n",
    "    similar_users = np.argsort(user_similarities)[::-1]  # Sort indices by similarity in descending order\n",
    "\n",
    "    # Exclude the target user from the similar users list (if present)\n",
    "    similar_users = similar_users[similar_users != user_id]\n",
    "\n",
    "    start = 0\n",
    "    while start < len(similar_users):\n",
    "        if np.isnan(user_similarities[similar_users[start]]):\n",
    "            start += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    non_nan_similar_users = similar_users[start:]\n",
    "\n",
    "\n",
    "    # Filter out users who haven't rated the item\n",
    "    rated_users = [user for user in non_nan_similar_users if not np.isnan(ratings[user, item_id])]\n",
    "\n",
    "    top_n_rated_users = rated_users[:N]  # Select top N similar users\n",
    "\n",
    "    if len(top_n_rated_users) < N:\n",
    "        print(\"(tag_id:\",item_id,\")\",\"Less similar users for N: \",N,\" \")\n",
    "        return np.nanmean(item_ratings)\n",
    "\n",
    "    # Get the ratings from the top N similar users and the corresponding similarities\n",
    "    top_n_ratings = item_ratings[top_n_rated_users]\n",
    "    top_n_similarities = user_similarities[top_n_rated_users]\n",
    "\n",
    "    if type == \"A\":\n",
    "        return np.sum(top_n_ratings) / N\n",
    "\n",
    "    # Calculate the weighted sum of the ratings\n",
    "    weighted_ratings_sum = np.dot(top_n_ratings, top_n_similarities)\n",
    "\n",
    "    # Calculate the sum of the absolute values of the similarities\n",
    "    similarity_sum = np.sum(top_n_similarities)\n",
    "\n",
    "    # Return the weighted average as the predicted rating\n",
    "    if similarity_sum != 0:\n",
    "        predicted_rating = weighted_ratings_sum / similarity_sum\n",
    "    else:\n",
    "        print(\"1\")\n",
    "        predicted_rating = 0\n",
    "\n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with k = 2: 0.7006938793917289,2243\n",
      "Loss with k = 3: 0.690393644492957,2243\n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "Loss with k = 5: 0.6769631382163931,2243\n"
     ]
    }
   ],
   "source": [
    "for k in {2, 3, 5}:  # Different values of N (number of similar users to consider)\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for i in range(tag_cutoff, num_tags):  # Iterate through items\n",
    "     for x in range(user_cutoff, num_users):  # Iterate through users\n",
    "            if np.isnan(utility_matrix[x][i]):  # Skip if the user hasn't rated the item\n",
    "                continue\n",
    "            else:\n",
    "                true_value = utility_matrix[x][i]  # Actual rating\n",
    "                ans = predict_rating(x, i, utility_matrix, similarity_matrix, k, \"A\", user_cutoff)  # Predict rating\n",
    "                \n",
    "                if np.isnan(ans): \n",
    "                    print(x, i, 'Hello')\n",
    "                    continue  # Skip NaN predictions to avoid errors\n",
    "                \n",
    "                # Calculate squared error\n",
    "                loss += (true_value - ans) ** 2\n",
    "                cnt += 1\n",
    "    \n",
    "    # Calculate and print RMSE (Root Mean Squared Error) for the current value of k\n",
    "    print(f\"Loss with k = {k}: {np.sqrt(loss / cnt)},{cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with k = 2: 1.0257074115306568\n",
      "Loss with k = 3: 0.7457370435666525\n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "Loss with k = 5: 0.6830546662035217\n"
     ]
    }
   ],
   "source": [
    "for k in {2, 3, 5}:  # Different values of N (number of similar users to consider)\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for i in range(tag_cutoff, num_tags):  # Iterate through items\n",
    "     for x in range(user_cutoff, num_users):  # Iterate through users\n",
    "            if np.isnan(utility_matrix[x][i]):  # Skip if the user hasn't rated the item\n",
    "                continue\n",
    "            else:\n",
    "                true_value = utility_matrix[x][i]  # Actual rating\n",
    "                ans = predict_rating(x, i, utility_matrix, similarity_matrix, k, \"B\", user_cutoff)  # Predict rating\n",
    "                \n",
    "                if np.isnan(ans): \n",
    "                    print(x, i, 'Hello')\n",
    "                    continue  # Skip NaN predictions to avoid errors\n",
    "                \n",
    "                # Calculate squared error\n",
    "                loss += (true_value - ans) ** 2\n",
    "                cnt += 1\n",
    "    \n",
    "    # Calculate and print RMSE (Root Mean Squared Error) for the current value of k\n",
    "    print(f\"Loss with k = {k}: {np.sqrt(loss / cnt)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### Question 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class MatrixFactorizationSGD:\n",
    "    def __init__(self, R, K, alpha=0.0005, epochs=10, lambda1=0, lambda2=0):\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K  # Number of latent factors\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.epochs = epochs  # Number of full passes (epochs)\n",
    "        self.lambda1 = lambda1  # Regularization parameter for P\n",
    "        self.lambda2 = lambda2  # Regularization parameter for Q\n",
    "        self.P = np.random.normal(scale=1./K, size=(self.num_users, K))\n",
    "        self.Q = np.random.normal(scale=1./K, size=(self.num_items, K))\n",
    "        # self.P = np.random.rand(self.num_users, K)  # User-feature matrix\n",
    "        # self.Q = np.random.rand(self.num_items, K)  # Item-feature matrix\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the matrix factorization model using stochastic gradient descent (SGD) over multiple epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(self.num_users):\n",
    "                for j in range(self.num_items):\n",
    "                    if not np.isnan(self.R[i][j]):  # Skip NaN values\n",
    "                        # Compute the prediction error\n",
    "                        eij = self.R[i][j] - np.dot(self.P[i, :], self.Q[j, :])\n",
    "\n",
    "                        # Update P and Q matrices with regularization terms lambda1 and lambda2\n",
    "                        for k in range(self.K):\n",
    "                            self.P[i][k] += self.alpha * (2 * eij * self.Q[j][k] - 2 * self.lambda1 * self.P[i][k])\n",
    "                            self.Q[j][k] += self.alpha * (2 * eij * self.P[i][k] - 2 * self.lambda2 * self.Q[j][k])\n",
    "            \n",
    "            # Compute and print the loss at the end of each epoch\n",
    "            loss = self.compute_loss()\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}: Loss = {loss}\")\n",
    "        \n",
    "        return self.P, self.Q\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \"\"\"\n",
    "        Compute the cost function (loss) with two regularization parameters.\n",
    "        The loss includes the squared error and regularization terms.\n",
    "        \"\"\"\n",
    "        predicted_R = np.dot(self.P, self.Q.T)\n",
    "        loss = 0\n",
    "        num_non_nan_entries = 0\n",
    "\n",
    "        for i in range(self.num_users):\n",
    "            for j in range(self.num_items):\n",
    "                if not np.isnan(self.R[i][j]):  # Only consider non-NaN values\n",
    "                    error = self.R[i][j] - predicted_R[i][j]\n",
    "                    loss += error ** 2\n",
    "                    num_non_nan_entries += 1\n",
    "        \n",
    "        # Adding regularization terms for P and Q matrices\n",
    "        loss += self.lambda1 * np.sum(self.P ** 2)  # Regularization for P\n",
    "        loss += self.lambda2 * np.sum(self.Q ** 2)  # Regularization for Q\n",
    "\n",
    "        # Normalize the loss by the number of non-NaN entries\n",
    "        loss /= num_non_nan_entries\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict(self, test_matrix):\n",
    "        \"\"\"\n",
    "        Predict the rating matrix for the test set.\n",
    "        \"\"\"\n",
    "        predicted_R = np.dot(self.P, self.Q.T)\n",
    "        predicted_R_test = np.copy(test_matrix)\n",
    "        \n",
    "        # Only fill the predictions where test_matrix is not NaN\n",
    "        for i in range(test_matrix.shape[0]):\n",
    "            for j in range(test_matrix.shape[1]):\n",
    "                if test_matrix[i][j] is not None:\n",
    "                    predicted_R_test[i][j] = predicted_R[i][j]\n",
    "\n",
    "        return predicted_R_test\n",
    "\n",
    "    def rmse(self, predicted_R, test_matrix):\n",
    "        \"\"\"\n",
    "        Compute the Root Mean Square Error (RMSE) between the predicted and actual ratings in the test matrix.\n",
    "        \"\"\"\n",
    "        mask = ~np.isnan(test_matrix)\n",
    "        actual_ratings = test_matrix[mask]\n",
    "        predicted_ratings = predicted_R[mask]\n",
    "        return np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "\n",
    "# Test the updated modular code with train and test matrices\n",
    "def run_tests():\n",
    "    # Assume train_matrix and test_matrix are predefined\n",
    "    R_train = train_matrix\n",
    "    R_test = test_matrix\n",
    "\n",
    "    # Parameters\n",
    "    latent_factors = [2, 5, 10]\n",
    "    alpha = 0.0005\n",
    "    epochs = 25\n",
    "    lambda1List = [0, 0.001, 0.05, 0.5]  # Regularization parameter for P\n",
    "    lambda2List = [0, 0.003, 0.05, 0.75]  # Regularization parameter for Q\n",
    "\n",
    "    for K in latent_factors:\n",
    "        for lambda1, lambda2 in zip(lambda1List, lambda2List):\n",
    "            mf_reg = MatrixFactorizationSGD(R_train, K, alpha, epochs, lambda1=lambda1, lambda2=lambda2)\n",
    "            P_reg, Q_reg = mf_reg.train()\n",
    "\n",
    "            # Predict for the training matrix\n",
    "            predicted_R_train = np.dot(P_reg, Q_reg.T)\n",
    "            train_rmse = mf_reg.rmse(predicted_R_train, R_train)\n",
    "\n",
    "            # Predict for the test matrix\n",
    "            predicted_R_reg = mf_reg.predict(R_test)\n",
    "            rmse_reg = mf_reg.rmse(predicted_R_reg, R_test)\n",
    "\n",
    "            print(f\"Latent Factors: {K}\")\n",
    "            print(f\"RMSE on train data with regularization (lambda1={lambda1}, lambda2={lambda2}): {train_rmse}\")\n",
    "            print(f\"RMSE on test data with regularization (lambda1={lambda1}, lambda2={lambda2}): {rmse_reg}\")\n",
    "\n",
    "# Example usage:\n",
    "run_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, KNNBaseline\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_surprise_df(matrix):\n",
    "    df = matrix.stack().reset_index()\n",
    "    df.columns = ['user', 'item', 'rating']\n",
    "    return df[pd.notna(df['rating'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surprise_knn(train_matrix, test_matrix, sim_options, N_values):\n",
    "    train_df = matrix_to_surprise_df(train_matrix)\n",
    "    print(train_df)\n",
    "    test_df = matrix_to_surprise_df(test_matrix)\n",
    "    print(test_df)\n",
    "    \n",
    "    reader = Reader(rating_scale=(0, max(train_matrix.max().max(), test_matrix.max().max())))\n",
    "    train_data = Dataset.load_from_df(train_df[['user', 'item', 'rating']], reader)\n",
    "    trainset = train_data.build_full_trainset()\n",
    "    testset = list(test_df.itertuples(index=False, name=None))\n",
    "\n",
    "    rmse_results = {}\n",
    "    \n",
    "    for N in N_values:\n",
    "        sim_options['k'] = N\n",
    "        algo = KNNBaseline(k=N, sim_options=sim_options)\n",
    "        algo.fit(trainset)\n",
    "        predictions = algo.test(testset)\n",
    "        rmse = accuracy.rmse(predictions, verbose=False)\n",
    "        rmse_results[N] = rmse\n",
    "\n",
    "    return rmse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options_item_item = {'name': 'pearson_baseline', 'user_based': False}  # Item-Item\n",
    "sim_options_user_user = {'name': 'pearson_baseline', 'user_based': True}   # User-User\n",
    "N_values = [2, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          user  item  rating\n",
      "0            4     1     4.0\n",
      "1            4     4     2.0\n",
      "2            4     7     2.0\n",
      "3            4     8     5.0\n",
      "4            4     9     5.0\n",
      "...        ...   ...     ...\n",
      "118222  379622  2845     0.0\n",
      "118223  379622  2876     0.0\n",
      "118224  379622  3018     0.0\n",
      "118225  379622  3199     0.0\n",
      "118226  379622  3242     0.0\n",
      "\n",
      "[118227 rows x 3 columns]\n",
      "        user  item  rating\n",
      "0     121035  3393     0.0\n",
      "1     121035  3586     0.0\n",
      "2     121035  3631     0.0\n",
      "3     121035  3757     0.0\n",
      "4     121035  3761     0.0\n",
      "...      ...   ...     ...\n",
      "2238  379622  4094     0.0\n",
      "2239  379622  4095     0.0\n",
      "2240  379622  4164     0.0\n",
      "2241  379622  4206     0.0\n",
      "2242  379622  4533     0.0\n",
      "\n",
      "[2243 rows x 3 columns]\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "rmse_item_item_surprise = surprise_knn(train_df, test_df, sim_options_item_item, N_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surprise Library RMSE Results (Tag-Tag):\n",
      "N=2: RMSE=0.7669281403985956\n",
      "N=3: RMSE=0.7275695777976227\n",
      "N=5: RMSE=0.694168988415908\n"
     ]
    }
   ],
   "source": [
    "print(\"Surprise Library RMSE Results (Tag-Tag):\")\n",
    "for N in rmse_item_item_surprise:\n",
    "    print(f\"N={N}: RMSE={rmse_item_item_surprise[N]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
