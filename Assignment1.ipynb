{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment1: Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Replace 'file_path.csv' with the actual path to your CSV file\n",
    "posts_df = pd.read_csv('./csv/Posts.csv')\n",
    "Tags = pd.read_csv('./csv/Tags.csv')\n",
    "\n",
    "answers_df = posts_df[posts_df['PostTypeId'] == 2][['Id', 'OwnerUserId', 'ParentId']]\n",
    "answers_df = answers_df.drop_duplicates(subset=['OwnerUserId','ParentId'])\n",
    "answers_df['ParentId'] = answers_df['ParentId'].astype(int)\n",
    "\n",
    "# Step 4: Extract the questions with their tags (where 'PostTypeId == 1')\n",
    "questions_df = posts_df[posts_df['PostTypeId'] == 1][['Id', 'Tags']]\n",
    "\n",
    "# Step 5: Ensure the question Ids are also of type int64\n",
    "questions_df['Id'] = questions_df['Id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 users with the most answers:\n",
      "       OwnerUserId  AnswerCount\n",
      "3189        9113.0         2838\n",
      "19912     177980.0         2318\n",
      "557         1204.0         2042\n",
      "\n",
      "Top 3 most used tags:\n",
      "    TagName  Count\n",
      "259  design   5162\n",
      "114      c#   4931\n",
      "37     java   4929\n"
     ]
    }
   ],
   "source": [
    "answerers_table = answers_df.groupby('OwnerUserId').size().reset_index(name='AnswerCount')\n",
    "top_answerers = answerers_table.sort_values(by='AnswerCount', ascending=False).head(3)\n",
    "\n",
    "# Group tags to find the tags with the highest count\n",
    "top_tags = Tags[['TagName', 'Count']].sort_values(by='Count', ascending=False).head(3)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 3 users with the most answers:\")\n",
    "print(top_answerers)\n",
    "\n",
    "print(\"\\nTop 3 most used tags:\")\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1 : First attach corresponding tags for answers by table join with questions table using Parent Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualified Answerers: Index([6.0, 11.0, 14.0, 15.0], dtype='float64', name='OwnerUserId')\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Merge answers with the corresponding tags from the question (use ParentId to match question Id)\n",
    "merged_df = pd.merge(answers_df, questions_df, left_on='ParentId', right_on='Id', suffixes=('_answer', '_question'))\n",
    "\n",
    "# Step 4: Select relevant columns\n",
    "filtered_answers_df = merged_df[['Id_answer', 'OwnerUserId', 'Tags','ParentId']]  # 'Tags' here are from the question\n",
    "filtered_answers_df = filtered_answers_df.drop_duplicates(['OwnerUserId','ParentId'])\n",
    "answerer_counts = filtered_answers_df.groupby('OwnerUserId').size()\n",
    "qualified_answerers = answerer_counts[answerer_counts >= 20].index\n",
    "print(\"Qualified Answerers:\", qualified_answerers[1:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step2 : Filter the answers using the qualified answers ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered answrers:    Id_answer  OwnerUserId                      Tags  ParentId\n",
      "0          3         11.0  |comments|anti-patterns|         1\n",
      "2         13          4.0  |comments|anti-patterns|         1\n",
      "3         56         17.0  |comments|anti-patterns|         1\n",
      "6        482        148.0  |comments|anti-patterns|         1\n",
      "8       1680        552.0  |comments|anti-patterns|         1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_answers = filtered_answers_df[filtered_answers_df['OwnerUserId'].isin(qualified_answerers)]\n",
    "print(\"filtered answrers:\" , filtered_answers.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step3 :  Filter tags and expand the answers tables by expanding rows for each tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualified Tags: 974\n"
     ]
    }
   ],
   "source": [
    "qualified_tags = Tags[Tags['Count'] >= 20]['Id']\n",
    "print(\"Qualified Tags:\", len(qualified_tags))\n",
    "\n",
    "tag_dict = Tags.set_index('TagName')['Id'].to_dict()\n",
    "\n",
    "tags_expanded = filtered_answers.copy()\n",
    "\n",
    "tags_expanded['Tags'] = tags_expanded['Tags'].str.split('|').apply(lambda x: x[1:-1])\n",
    "tags_expanded = tags_expanded.explode('Tags')\n",
    "tags_expanded['Tags'] = tags_expanded['Tags'].map(tag_dict)\n",
    "tags_expanded = tags_expanded[tags_expanded['Tags'].isin(qualified_tags)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step4 : Create Utility matrix from the filtered answers table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert Matrix: Tags         1.0     3.0     4.0     7.0     8.0     9.0     11.0    12.0    \\\n",
      "OwnerUserId                                                                   \n",
      "4.0            13.0     NaN     6.0     6.0    61.0    55.0     8.0     3.0   \n",
      "6.0             NaN     NaN     8.0     NaN     6.0     4.0     1.0     2.0   \n",
      "11.0            1.0     NaN     1.0     NaN     NaN     1.0     NaN     1.0   \n",
      "14.0            NaN     NaN     1.0     NaN     1.0     1.0     NaN     1.0   \n",
      "15.0            1.0     NaN     2.0     1.0     4.0     4.0     1.0     1.0   \n",
      "...             ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "356695.0        NaN     NaN     NaN     NaN     NaN     1.0     NaN     NaN   \n",
      "366014.0        NaN     NaN     NaN     NaN     NaN     NaN     1.0     NaN   \n",
      "373864.0        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "378329.0        1.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "379622.0        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "\n",
      "Tags         13.0    14.0    ...  4639.0  4646.0  4661.0  4682.0  4683.0  \\\n",
      "OwnerUserId                  ...                                           \n",
      "4.0             NaN     NaN  ...     NaN     NaN     NaN     2.0     1.0   \n",
      "6.0             NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "11.0            NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "14.0            NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "15.0            NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "...             ...     ...  ...     ...     ...     ...     ...     ...   \n",
      "356695.0        NaN     NaN  ...     NaN     NaN     1.0     NaN     NaN   \n",
      "366014.0        NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "373864.0        NaN     NaN  ...     NaN     NaN     2.0     NaN     NaN   \n",
      "378329.0        NaN     NaN  ...     NaN     NaN     1.0     NaN     NaN   \n",
      "379622.0        NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "\n",
      "Tags         4687.0  4690.0  4704.0  4720.0  4750.0  \n",
      "OwnerUserId                                          \n",
      "4.0             1.0     NaN     NaN     1.0     NaN  \n",
      "6.0             NaN     NaN     NaN     NaN     NaN  \n",
      "11.0            NaN     NaN     NaN     NaN     NaN  \n",
      "14.0            NaN     NaN     NaN     NaN     NaN  \n",
      "15.0            NaN     NaN     NaN     NaN     NaN  \n",
      "...             ...     ...     ...     ...     ...  \n",
      "356695.0        NaN     NaN     NaN     NaN     NaN  \n",
      "366014.0        NaN     NaN     NaN     NaN     NaN  \n",
      "373864.0        NaN     1.0     NaN     NaN     NaN  \n",
      "378329.0        NaN     NaN     NaN     5.0     1.0  \n",
      "379622.0        NaN     NaN     NaN     NaN     NaN  \n",
      "\n",
      "[1160 rows x 973 columns]\n",
      "Dimensions of the Expert matrix: (1160, 973)\n"
     ]
    }
   ],
   "source": [
    "expert_matrix = pd.pivot_table(\n",
    "    tags_expanded, \n",
    "    index='OwnerUserId', \n",
    "    columns='Tags', \n",
    "    aggfunc='size', \n",
    "    fill_value=np.nan\n",
    ")\n",
    "\n",
    "all_qualified_tags = pd.Series(qualified_tags, name='Tags')\n",
    "# expert_matrix = expert_matrix.reindex(columns=all_qualified_tags, fill_value=np.nan)\n",
    "print(\"Expert Matrix:\", expert_matrix)\n",
    "# dimensions = utility_matrix_sorted.shape\n",
    "print(\"Dimensions of the Expert matrix:\", expert_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step5: convert to numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13. nan  6. ... nan  1. nan]\n",
      " [nan nan  8. ... nan nan nan]\n",
      " [ 1. nan  1. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 1. nan nan ... nan  5.  1.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "expert_matrix = expert_matrix.to_numpy()\n",
    "print(expert_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Normalize the utility matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "normalize = lambda x: np.nan if np.isnan(x) else (np.floor(x / 3) if x < 15 else 5)\n",
    "vectorized_modify_entries = np.vectorize(normalize)\n",
    "utility_matrix = vectorized_modify_entries(expert_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4. nan  2. ... nan  0. nan]\n",
      " [nan nan  2. ... nan nan nan]\n",
      " [ 0. nan  0. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 0. nan nan ... nan  1.  0.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(utility_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility Matrix Metrics:\n",
      "Summation value of the utility matrix: 41180.0\n",
      "Highest row sum of the utility matrix: 1162.0\n",
      "Highest column sum of the utility matrix: 1403.0\n"
     ]
    }
   ],
   "source": [
    "sum_utility_matrix = np.nansum(utility_matrix)\n",
    "highest_row_sum = np.max(np.nansum(utility_matrix, axis=1))\n",
    "highest_column_sum = np.max(np.nansum(utility_matrix, axis=0))\n",
    "\n",
    "print(\"Utility Matrix Metrics:\")\n",
    "print(\"Summation value of the utility matrix:\", sum_utility_matrix)\n",
    "print(\"Highest row sum of the utility matrix:\", highest_row_sum)\n",
    "print(\"Highest column sum of the utility matrix:\", highest_column_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Create Test matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986 827\n",
      "23161\n",
      "2243\n",
      "Test Matrix Metrics:\n",
      "dimensions:  (174, 146)\n",
      "Summation value of the utility matrix: 642.0\n",
      "Highest row sum of the utility matrix: 136.0\n",
      "Highest column sum of the utility matrix: 96.0\n"
     ]
    }
   ],
   "source": [
    "num_users, num_tags = utility_matrix.shape\n",
    "user_cutoff = int(num_users * 0.85)\n",
    "tag_cutoff = int(num_tags * 0.85)\n",
    "\n",
    "print(user_cutoff,tag_cutoff)\n",
    "\n",
    "test_matrix = utility_matrix[user_cutoff:, tag_cutoff:]\n",
    "sum_test_matrix = np.nansum(test_matrix)\n",
    "highest_row_sum = np.max(np.nansum(test_matrix, axis=1))\n",
    "highest_column_sum = np.max(np.nansum(test_matrix, axis=0))\n",
    "\n",
    "print(np.count_nonzero(np.isnan(test_matrix)))\n",
    "print(np.count_nonzero(~np.isnan(test_matrix)))\n",
    "\n",
    "print(\"Test Matrix Metrics:\")\n",
    "print(\"dimensions: \",test_matrix.shape)\n",
    "print(\"Summation value of the utility matrix:\", sum_test_matrix)\n",
    "print(\"Highest row sum of the utility matrix:\", highest_row_sum)\n",
    "print(\"Highest column sum of the utility matrix:\", highest_column_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = utility_matrix.copy()\n",
    "\n",
    "train_matrix[user_cutoff:, tag_cutoff:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40538.0\n"
     ]
    }
   ],
   "source": [
    "train_matrix_sum = np.nansum(train_matrix)\n",
    "print(train_matrix_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag-Tag Recommandation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = utility_matrix[:user_cutoff]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Center the data by subtracting the mean of each item (column)\n",
    "# Subtract column means from each value\n",
    "df_centered = df.sub(df.mean(axis=1), axis=0)\n",
    "\n",
    "# Pearson correlation matrix: compute pairwise correlation between items\n",
    "similarity_matrix = df_centered.corr(method='pearson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, item_id, ratings, similarity_matrix, N , type):\n",
    "    \n",
    "    # Get the similarity scores for the target item with all other items\n",
    "    item_similarities = np.array(similarity_matrix[item_id][:tag_cutoff])\n",
    "    \n",
    "    # Get the ratings of user_id for all items\n",
    "    user_ratings = np.array(ratings[user_id])\n",
    "    \n",
    "    # Sort the similarities and take the top N most similar items (excluding the target item itself)\n",
    "    similar_items = np.argsort(item_similarities)[::-1]  # Sort indices by similarity in descending order\n",
    "\n",
    "    start = 0\n",
    "    while start < len(similar_items):\n",
    "        if np.isnan(item_similarities[similar_items[start]]): start += 1\n",
    "        else : break\n",
    "    \n",
    "    non_nan_similar_items = similar_items[start:]\n",
    "\n",
    "    # Filter out items the user hasn't rated\n",
    "    rated_items = [item for item in non_nan_similar_items \n",
    "                         if not np.isnan(ratings[user_id][item])]\n",
    "    \n",
    "    top_n_rated_items = rated_items[:N]  # Select top N similar items\n",
    "\n",
    "\n",
    "    if len(top_n_rated_items) < N:\n",
    "        return np.nanmean(user_ratings)\n",
    "    \n",
    "    # Get the user's ratings for the top N similar items and the corresponding similarities\n",
    "    top_n_ratings = user_ratings[top_n_rated_items]\n",
    "    top_n_similarities = item_similarities[top_n_rated_items]\n",
    "    \n",
    "    if type == \"A\":\n",
    "     return np.sum(top_n_ratings) / N\n",
    "\n",
    "    # Calculate the weighted sum of the ratings\n",
    "    weighted_ratings_sum = np.dot(top_n_ratings, top_n_similarities)\n",
    "    \n",
    "    # Calculate the sum of the absolute values of the similarities\n",
    "\n",
    "    similarity_sum = np.sum(top_n_similarities)\n",
    "    \n",
    "    # Return the weighted average as the predicted rating\n",
    "    if similarity_sum != 0 : \n",
    "        predicted_rating = weighted_ratings_sum / similarity_sum\n",
    "    else : \n",
    "        print(\"1\")\n",
    "        predicted_rating = 0\n",
    "    \n",
    "    return predicted_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with k=  2 : 0.8368331915377201\n",
      "Loss with k=  3 : 0.8068537836476033\n",
      "Loss with k=  5 : 0.7667057566908195\n"
     ]
    }
   ],
   "source": [
    "for k in {2,3,5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x in range(user_cutoff,num_users):\n",
    "        for i in range(tag_cutoff,num_tags):\n",
    "            if np.isnan(utility_matrix[x][i]): continue\n",
    "            else : \n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x,i,utility_matrix,similarity_matrix,k,\"A\")\n",
    "                if(np.isnan(ans)) : \n",
    "                    print(x,i,'Hello')\n",
    "                loss = loss + (true_value - ans)*(true_value - ans)\n",
    "                cnt += 1\n",
    "    print(\"Loss with k= \",k,\":\",np.sqrt(loss / cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with k=  2 : 0.8368997316823331\n",
      "Loss with k=  3 : 0.8066557194070604\n",
      "Loss with k=  5 : 0.768163144485662\n"
     ]
    }
   ],
   "source": [
    "for k in {2,3,5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x in range(user_cutoff,num_users):\n",
    "        for i in range(tag_cutoff,num_tags):\n",
    "            if np.isnan(utility_matrix[x][i]): continue\n",
    "            else : \n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x,i,utility_matrix,similarity_matrix,k,\"B\")\n",
    "                if(np.isnan(ans)) : \n",
    "                    print(x,i,'Hello')\n",
    "                loss = loss + (true_value - ans)*(true_value - ans)\n",
    "                cnt += 1\n",
    "    print(\"Loss with k= \",k,\":\",np.sqrt(loss / cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-User Recommandation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1160, 827)\n",
      "(1160, 1160)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = utility_matrix[:, :tag_cutoff]\n",
    "print(data.shape)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Center the data by subtracting the mean of each item (column)\n",
    "# Subtract column means from each value\n",
    "df_centered = df.sub(df.mean(axis=0), axis=1)\n",
    "\n",
    "# Pearson correlation matrix: compute pairwise correlation between items\n",
    "similarity_matrix = df_centered.T.corr(method='pearson')\n",
    "print(similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, item_id, ratings, similarity_matrix, N, type, user_cutoff):\n",
    "    # Get the similarity scores for the target user with all other users within the user_cutoff\n",
    "    user_similarities = np.array(similarity_matrix[user_id][:user_cutoff])\n",
    "\n",
    "    # Get the ratings for the item by all users within the user_cutoff\n",
    "    item_ratings = np.array(ratings[:,item_id])\n",
    "\n",
    "    # Sort the similarities and take the top N most similar users (excluding the target user itself)\n",
    "    similar_users = np.argsort(user_similarities)[::-1]  # Sort indices by similarity in descending order\n",
    "\n",
    "    # Exclude the target user from the similar users list (if present)\n",
    "    similar_users = similar_users[similar_users != user_id]\n",
    "\n",
    "    start = 0\n",
    "    while start < len(similar_users):\n",
    "        if np.isnan(user_similarities[similar_users[start]]):\n",
    "            start += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    non_nan_similar_users = similar_users[start:]\n",
    "\n",
    "\n",
    "    # Filter out users who haven't rated the item\n",
    "    rated_users = [user for user in non_nan_similar_users if not np.isnan(ratings[user, item_id])]\n",
    "\n",
    "    top_n_rated_users = rated_users[:N]  # Select top N similar users\n",
    "\n",
    "    if len(top_n_rated_users) < N:\n",
    "        print(\"(tag_id:\",item_id,\")\",\"Less similar users for N: \",N,\" \")\n",
    "        return np.nanmean(item_ratings)\n",
    "\n",
    "    # Get the ratings from the top N similar users and the corresponding similarities\n",
    "    top_n_ratings = item_ratings[top_n_rated_users]\n",
    "    top_n_similarities = user_similarities[top_n_rated_users]\n",
    "\n",
    "    if type == \"A\":\n",
    "        return np.sum(top_n_ratings) / N\n",
    "\n",
    "    # Calculate the weighted sum of the ratings\n",
    "    weighted_ratings_sum = np.dot(top_n_ratings, top_n_similarities)\n",
    "\n",
    "    # Calculate the sum of the absolute values of the similarities\n",
    "    similarity_sum = np.sum(top_n_similarities)\n",
    "\n",
    "    # Return the weighted average as the predicted rating\n",
    "    if similarity_sum != 0:\n",
    "        predicted_rating = weighted_ratings_sum / similarity_sum\n",
    "    else:\n",
    "        print(\"1\")\n",
    "        predicted_rating = 0\n",
    "\n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with k = 2: 0.7006938793917289,2243\n",
      "Loss with k = 3: 0.690393644492957,2243\n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "Loss with k = 5: 0.6769631382163931,2243\n"
     ]
    }
   ],
   "source": [
    "for k in {2, 3, 5}:  # Different values of N (number of similar users to consider)\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for i in range(tag_cutoff, num_tags):  # Iterate through items\n",
    "     for x in range(user_cutoff, num_users):  # Iterate through users\n",
    "            if np.isnan(utility_matrix[x][i]):  # Skip if the user hasn't rated the item\n",
    "                continue\n",
    "            else:\n",
    "                true_value = utility_matrix[x][i]  # Actual rating\n",
    "                ans = predict_rating(x, i, utility_matrix, similarity_matrix, k, \"A\", user_cutoff)  # Predict rating\n",
    "                \n",
    "                if np.isnan(ans): \n",
    "                    print(x, i, 'Hello')\n",
    "                    continue  # Skip NaN predictions to avoid errors\n",
    "                \n",
    "                # Calculate squared error\n",
    "                loss += (true_value - ans) ** 2\n",
    "                cnt += 1\n",
    "    \n",
    "    # Calculate and print RMSE (Root Mean Squared Error) for the current value of k\n",
    "    print(f\"Loss with k = {k}: {np.sqrt(loss / cnt)},{cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with k = 2: 1.0257074115306568\n",
      "Loss with k = 3: 0.7457370435666525\n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "Loss with k = 5: 0.6830546662035217\n"
     ]
    }
   ],
   "source": [
    "for k in {2, 3, 5}:  # Different values of N (number of similar users to consider)\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for i in range(tag_cutoff, num_tags):  # Iterate through items\n",
    "     for x in range(user_cutoff, num_users):  # Iterate through users\n",
    "            if np.isnan(utility_matrix[x][i]):  # Skip if the user hasn't rated the item\n",
    "                continue\n",
    "            else:\n",
    "                true_value = utility_matrix[x][i]  # Actual rating\n",
    "                ans = predict_rating(x, i, utility_matrix, similarity_matrix, k, \"B\", user_cutoff)  # Predict rating\n",
    "                \n",
    "                if np.isnan(ans): \n",
    "                    print(x, i, 'Hello')\n",
    "                    continue  # Skip NaN predictions to avoid errors\n",
    "                \n",
    "                # Calculate squared error\n",
    "                loss += (true_value - ans) ** 2\n",
    "                cnt += 1\n",
    "    \n",
    "    # Calculate and print RMSE (Root Mean Squared Error) for the current value of k\n",
    "    print(f\"Loss with k = {k}: {np.sqrt(loss / cnt)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### Question 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: Loss = 1.023128928450721\n",
      "Epoch 2/25: Loss = 1.0028482661049336\n",
      "Epoch 3/25: Loss = 0.9884285604466655\n",
      "Epoch 4/25: Loss = 0.9777729395364401\n",
      "Epoch 5/25: Loss = 0.9696435494786823\n",
      "Epoch 6/25: Loss = 0.9632711145922408\n",
      "Epoch 7/25: Loss = 0.9581566022932247\n",
      "Epoch 8/25: Loss = 0.953963926445126\n",
      "Epoch 9/25: Loss = 0.9504587467359011\n",
      "Epoch 10/25: Loss = 0.9474718801175241\n",
      "Epoch 11/25: Loss = 0.9448764067112378\n",
      "Epoch 12/25: Loss = 0.9425726105675912\n",
      "Epoch 13/25: Loss = 0.9404774369283352\n",
      "Epoch 14/25: Loss = 0.9385164693470898\n",
      "Epoch 15/25: Loss = 0.9366171277482176\n",
      "Epoch 16/25: Loss = 0.9347021499317748\n",
      "Epoch 17/25: Loss = 0.9326825914238297\n",
      "Epoch 18/25: Loss = 0.9304496463618708\n",
      "Epoch 19/25: Loss = 0.9278646190163394\n",
      "Epoch 20/25: Loss = 0.924746435734811\n",
      "Epoch 21/25: Loss = 0.920856298387833\n",
      "Epoch 22/25: Loss = 0.9158796467167554\n",
      "Epoch 23/25: Loss = 0.9094068567999303\n",
      "Epoch 24/25: Loss = 0.9009165522533594\n",
      "Epoch 25/25: Loss = 0.8897695948491565\n",
      "Latent Factors: 2\n",
      "RMSE on train data with regularization (lambda1=0, lambda2=0): 0.943275990815606\n",
      "RMSE on test data with regularization (lambda1=0, lambda2=0): 0.8621217329402591\n",
      "Epoch 1/25: Loss = 1.0146237628000432\n",
      "Epoch 2/25: Loss = 0.9974359077176916\n",
      "Epoch 3/25: Loss = 0.9847712227607117\n",
      "Epoch 4/25: Loss = 0.9750570060256564\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 120\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSE on test data with regularization (lambda1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlambda1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, lambda2=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlambda2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse_reg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43mrun_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 105\u001b[0m, in \u001b[0;36mrun_tests\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lambda1, lambda2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lambda1List, lambda2List):\n\u001b[1;32m    104\u001b[0m     mf_reg \u001b[38;5;241m=\u001b[39m MatrixFactorizationSGD(R_train, K, alpha, epochs, lambda1\u001b[38;5;241m=\u001b[39mlambda1, lambda2\u001b[38;5;241m=\u001b[39mlambda2)\n\u001b[0;32m--> 105\u001b[0m     P_reg, Q_reg \u001b[38;5;241m=\u001b[39m \u001b[43mmf_reg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Predict for the training matrix\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     predicted_R_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(P_reg, Q_reg\u001b[38;5;241m.\u001b[39mT)\n",
      "Cell \u001b[0;32mIn[40], line 35\u001b[0m, in \u001b[0;36mMatrixFactorizationSGD.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ[j][k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m eij \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP[i][k] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda2 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ[j][k])\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Compute and print the loss at the end of each epoch\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ\n",
      "Cell \u001b[0;32mIn[40], line 51\u001b[0m, in \u001b[0;36mMatrixFactorizationSGD.compute_loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_users):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_items):\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mR\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# Only consider non-NaN values\u001b[39;00m\n\u001b[1;32m     52\u001b[0m             error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR[i][j] \u001b[38;5;241m-\u001b[39m predicted_R[i][j]\n\u001b[1;32m     53\u001b[0m             loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m error \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class MatrixFactorizationSGD:\n",
    "    def __init__(self, R, K, alpha=0.0005, epochs=10, lambda1=0, lambda2=0):\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K  # Number of latent factors\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.epochs = epochs  # Number of full passes (epochs)\n",
    "        self.lambda1 = lambda1  # Regularization parameter for P\n",
    "        self.lambda2 = lambda2  # Regularization parameter for Q\n",
    "        self.P = np.random.normal(scale=1./K, size=(self.num_users, K))\n",
    "        self.Q = np.random.normal(scale=1./K, size=(self.num_items, K))\n",
    "        # self.P = np.random.rand(self.num_users, K)  # User-feature matrix\n",
    "        # self.Q = np.random.rand(self.num_items, K)  # Item-feature matrix\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the matrix factorization model using stochastic gradient descent (SGD) over multiple epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(self.num_users):\n",
    "                for j in range(self.num_items):\n",
    "                    if not np.isnan(self.R[i][j]):  # Skip NaN values\n",
    "                        # Compute the prediction error\n",
    "                        eij = self.R[i][j] - np.dot(self.P[i, :], self.Q[j, :])\n",
    "\n",
    "                        # Update P and Q matrices with regularization terms lambda1 and lambda2\n",
    "                        for k in range(self.K):\n",
    "                            self.P[i][k] += self.alpha * (2 * eij * self.Q[j][k] - 2 * self.lambda1 * self.P[i][k])\n",
    "                            self.Q[j][k] += self.alpha * (2 * eij * self.P[i][k] - 2 * self.lambda2 * self.Q[j][k])\n",
    "            \n",
    "            # Compute and print the loss at the end of each epoch\n",
    "            loss = self.compute_loss()\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}: Loss = {loss}\")\n",
    "        \n",
    "        return self.P, self.Q\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \"\"\"\n",
    "        Compute the cost function (loss) with two regularization parameters.\n",
    "        The loss includes the squared error and regularization terms.\n",
    "        \"\"\"\n",
    "        predicted_R = np.dot(self.P, self.Q.T)\n",
    "        loss = 0\n",
    "        num_non_nan_entries = 0\n",
    "\n",
    "        for i in range(self.num_users):\n",
    "            for j in range(self.num_items):\n",
    "                if not np.isnan(self.R[i][j]):  # Only consider non-NaN values\n",
    "                    error = self.R[i][j] - predicted_R[i][j]\n",
    "                    loss += error ** 2\n",
    "                    num_non_nan_entries += 1\n",
    "        \n",
    "        # Adding regularization terms for P and Q matrices\n",
    "        loss += self.lambda1 * np.sum(self.P ** 2)  # Regularization for P\n",
    "        loss += self.lambda2 * np.sum(self.Q ** 2)  # Regularization for Q\n",
    "\n",
    "        # Normalize the loss by the number of non-NaN entries\n",
    "        loss /= num_non_nan_entries\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict(self, test_matrix):\n",
    "        \"\"\"\n",
    "        Predict the rating matrix for the test set.\n",
    "        \"\"\"\n",
    "        predicted_R = np.dot(self.P, self.Q.T)\n",
    "        predicted_R_test = np.copy(test_matrix)\n",
    "        \n",
    "        # Only fill the predictions where test_matrix is not NaN\n",
    "        for i in range(test_matrix.shape[0]):\n",
    "            for j in range(test_matrix.shape[1]):\n",
    "                if test_matrix[i][j] is not None:\n",
    "                    predicted_R_test[i][j] = predicted_R[i][j]\n",
    "\n",
    "        return predicted_R_test\n",
    "\n",
    "    def rmse(self, predicted_R, test_matrix):\n",
    "        \"\"\"\n",
    "        Compute the Root Mean Square Error (RMSE) between the predicted and actual ratings in the test matrix.\n",
    "        \"\"\"\n",
    "        mask = ~np.isnan(test_matrix)\n",
    "        actual_ratings = test_matrix[mask]\n",
    "        predicted_ratings = predicted_R[mask]\n",
    "        return np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "\n",
    "# Test the updated modular code with train and test matrices\n",
    "def run_tests():\n",
    "    # Assume train_matrix and test_matrix are predefined\n",
    "    R_train = train_matrix\n",
    "    R_test = test_matrix\n",
    "\n",
    "    # Parameters\n",
    "    latent_factors = [2, 5, 10]\n",
    "    alpha = 0.0005\n",
    "    epochs = 25\n",
    "    lambda1List = [0, 0.001, 0.05, 0.5]  # Regularization parameter for P\n",
    "    lambda2List = [0, 0.003, 0.05, 0.75]  # Regularization parameter for Q\n",
    "\n",
    "    for K in latent_factors:\n",
    "        for lambda1, lambda2 in zip(lambda1List, lambda2List):\n",
    "            mf_reg = MatrixFactorizationSGD(R_train, K, alpha, epochs, lambda1=lambda1, lambda2=lambda2)\n",
    "            P_reg, Q_reg = mf_reg.train()\n",
    "\n",
    "            # Predict for the training matrix\n",
    "            predicted_R_train = np.dot(P_reg, Q_reg.T)\n",
    "            train_rmse = mf_reg.rmse(predicted_R_train, R_train)\n",
    "\n",
    "            # Predict for the test matrix\n",
    "            predicted_R_reg = mf_reg.predict(R_test)\n",
    "            rmse_reg = mf_reg.rmse(predicted_R_reg, R_test)\n",
    "\n",
    "            print(f\"Latent Factors: {K}\")\n",
    "            print(f\"RMSE on train data with regularization (lambda1={lambda1}, lambda2={lambda2}): {train_rmse}\")\n",
    "            print(f\"RMSE on test data with regularization (lambda1={lambda1}, lambda2={lambda2}): {rmse_reg}\")\n",
    "\n",
    "# Example usage:\n",
    "run_tests()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
