{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment1: Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Replace 'file_path.csv' with the actual path to your CSV file\n",
    "posts_df = pd.read_csv('./csv/Posts.csv')\n",
    "Tags = pd.read_csv('./csv/Tags.csv')\n",
    "\n",
    "answers_df = posts_df[posts_df['PostTypeId'] == 2][['Id', 'OwnerUserId', 'ParentId']]\n",
    "answers_df['ParentId'] = answers_df['ParentId'].astype(int)\n",
    "\n",
    "# Step 4: Extract the questions with their tags (where 'PostTypeId == 1')\n",
    "questions_df = posts_df[posts_df['PostTypeId'] == 1][['Id', 'Tags']]\n",
    "\n",
    "# Step 5: Ensure the question Ids are also of type int64\n",
    "questions_df['Id'] = questions_df['Id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 users with the most answers:\n",
      "       OwnerUserId  AnswerCount\n",
      "3189        9113.0         2839\n",
      "19912     177980.0         2326\n",
      "557         1204.0         2043\n",
      "\n",
      "Top 3 most used tags:\n",
      "    TagName  Count\n",
      "259  design   5162\n",
      "114      c#   4931\n",
      "37     java   4929\n"
     ]
    }
   ],
   "source": [
    "answerers_table = answers_df.groupby('OwnerUserId').size().reset_index(name='AnswerCount')\n",
    "top_answerers = answerers_table.sort_values(by='AnswerCount', ascending=False).head(3)\n",
    "\n",
    "# Group tags to find the tags with the highest count\n",
    "top_tags = Tags[['TagName', 'Count']].sort_values(by='Count', ascending=False).head(3)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 3 users with the most answers:\")\n",
    "print(top_answerers)\n",
    "\n",
    "print(\"\\nTop 3 most used tags:\")\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1 : First attach corresponding tags for answers by table join with questions table using Parent Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualified Answerers: Index([6.0, 11.0, 14.0, 15.0], dtype='float64', name='OwnerUserId')\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Merge answers with the corresponding tags from the question (use ParentId to match question Id)\n",
    "merged_df = pd.merge(answers_df, questions_df, left_on='ParentId', right_on='Id', suffixes=('_answer', '_question'))\n",
    "\n",
    "# Step 4: Select relevant columns\n",
    "filtered_answers_df = merged_df[['Id_answer', 'OwnerUserId', 'Tags']]  # 'Tags' here are from the question\n",
    "answerer_counts = filtered_answers_df.groupby('OwnerUserId').size()\n",
    "qualified_answerers = answerer_counts[answerer_counts >= 20].index\n",
    "print(\"Qualified Answerers:\", qualified_answerers[1:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step2 : Filter the answers using the qualified answers ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered answrers:    Id_answer  OwnerUserId                                               Tags\n",
      "0          3         11.0                           |comments|anti-patterns|\n",
      "3         13          4.0                           |comments|anti-patterns|\n",
      "4         20          6.0                     |productivity|time-management|\n",
      "6         23         11.0                     |productivity|time-management|\n",
      "8         26         17.0  |business|project-management|development-process|\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_answers = filtered_answers_df[filtered_answers_df['OwnerUserId'].isin(qualified_answerers)]\n",
    "print(\"filtered answrers:\" , filtered_answers.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step3 :  Filter tags and expand the answers tables by expanding rows for each tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualified Tags: 974\n"
     ]
    }
   ],
   "source": [
    "qualified_tags = Tags[Tags['Count'] >= 20]['Id']\n",
    "print(\"Qualified Tags:\", len(qualified_tags))\n",
    "\n",
    "tag_dict = Tags.set_index('TagName')['Id'].to_dict()\n",
    "\n",
    "tags_expanded = filtered_answers.copy()\n",
    "tags_expanded['Tags'] = tags_expanded['Tags'].str.split('|').apply(lambda x: x[1:-1])\n",
    "tags_expanded = tags_expanded.explode('Tags')\n",
    "tags_expanded['Tags'] = tags_expanded['Tags'].map(tag_dict)\n",
    "tags_expanded = tags_expanded[tags_expanded['Tags'].isin(qualified_tags)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step4 : Create Utility matrix from the filtered answers table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert Matrix: Tags         1     3     4     7     8     9     11    12    13    14    ...  \\\n",
      "OwnerUserId                                                              ...   \n",
      "4.0          13.0   NaN   6.0   6.0  61.0  55.0   8.0   3.0   NaN   NaN  ...   \n",
      "6.0           NaN   NaN   8.0   NaN   6.0   4.0   1.0   2.0   NaN   NaN  ...   \n",
      "11.0          1.0   NaN   1.0   NaN   NaN   1.0   NaN   1.0   NaN   NaN  ...   \n",
      "14.0          NaN   NaN   1.0   NaN   1.0   1.0   NaN   1.0   NaN   NaN  ...   \n",
      "15.0          1.0   NaN   2.0   1.0   4.0   4.0   1.0   1.0   NaN   NaN  ...   \n",
      "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
      "356695.0      NaN   NaN   NaN   NaN   NaN   1.0   NaN   NaN   NaN   NaN  ...   \n",
      "366014.0      NaN   NaN   NaN   NaN   NaN   NaN   1.0   NaN   NaN   NaN  ...   \n",
      "373864.0      NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "378329.0      1.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "379622.0      NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "\n",
      "Tags         4639  4646  4661  4682  4683  4687  4690  4704  4720  4750  \n",
      "OwnerUserId                                                              \n",
      "4.0           NaN   NaN   NaN   2.0   1.0   1.0   NaN   NaN   1.0   NaN  \n",
      "6.0           NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "11.0          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "14.0          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "15.0          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "356695.0      NaN   NaN   1.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "366014.0      NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "373864.0      NaN   NaN   2.0   NaN   NaN   NaN   1.0   NaN   NaN   NaN  \n",
      "378329.0      NaN   NaN   1.0   NaN   NaN   NaN   NaN   NaN   5.0   1.0  \n",
      "379622.0      NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "\n",
      "[1163 rows x 974 columns]\n",
      "Dimensions of the Expert matrix: (1163, 974)\n"
     ]
    }
   ],
   "source": [
    "expert_matrix = pd.pivot_table(\n",
    "    tags_expanded, \n",
    "    index='OwnerUserId', \n",
    "    columns='Tags', \n",
    "    aggfunc='size', \n",
    "    fill_value=np.nan\n",
    ")\n",
    "\n",
    "all_qualified_tags = pd.Series(qualified_tags, name='Tags')\n",
    "expert_matrix = expert_matrix.reindex(columns=all_qualified_tags, fill_value=np.nan)\n",
    "print(\"Expert Matrix:\", expert_matrix)\n",
    "# dimensions = utility_matrix_sorted.shape\n",
    "print(\"Dimensions of the Expert matrix:\", expert_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step5: convert to numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13. nan  6. ... nan  1. nan]\n",
      " [nan nan  8. ... nan nan nan]\n",
      " [ 1. nan  1. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 1. nan nan ... nan  5.  1.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "expert_matrix = expert_matrix.to_numpy()\n",
    "print(expert_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Normalize the utility matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create the utility_matrix with the same shape as expert_matrix\n",
    "utility_matrix = expert_matrix\n",
    "\n",
    "# Vectorized condition\n",
    "utility_matrix[expert_matrix > 15] = 5\n",
    "mask = ~np.isnan(expert_matrix) & (expert_matrix <= 15)\n",
    "utility_matrix[mask] = expert_matrix[expert_matrix <= 15] // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4. nan  2. ... nan  0. nan]\n",
      " [nan nan  2. ... nan nan nan]\n",
      " [ 0. nan  0. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 0. nan nan ... nan  1.  0.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(utility_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility Matrix Metrics:\n",
      "Summation value of the utility matrix: nan\n",
      "Highest row sum of the utility matrix: nan\n",
      "Highest column sum of the utility matrix: nan\n"
     ]
    }
   ],
   "source": [
    "sum_utility_matrix = np.sum(utility_matrix)\n",
    "highest_row_sum = np.max(np.sum(utility_matrix, axis=1))\n",
    "highest_column_sum = np.max(np.sum(utility_matrix, axis=0))\n",
    "\n",
    "print(\"Utility Matrix Metrics:\")\n",
    "print(\"Summation value of the utility matrix:\", sum_utility_matrix)\n",
    "print(\"Highest row sum of the utility matrix:\", highest_row_sum)\n",
    "print(\"Highest column sum of the utility matrix:\", highest_column_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Create Test matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988 827\n",
      "Test Matrix Metrics:\n",
      "dimensions:  (175, 147)\n",
      "Summation value of the utility matrix: nan\n",
      "Highest row sum of the utility matrix: nan\n",
      "Highest column sum of the utility matrix: nan\n"
     ]
    }
   ],
   "source": [
    "num_users, num_tags = utility_matrix.shape\n",
    "user_cutoff = int(num_users * 0.85)\n",
    "tag_cutoff = int(num_tags * 0.85)\n",
    "\n",
    "print(user_cutoff,tag_cutoff)\n",
    "\n",
    "test_matrix = utility_matrix[user_cutoff:, tag_cutoff:]\n",
    "sum_test_matrix = np.sum(test_matrix)\n",
    "highest_row_sum = np.max(np.sum(test_matrix, axis=1))\n",
    "highest_column_sum = np.max(np.sum(test_matrix, axis=0))\n",
    "\n",
    "print(\"Test Matrix Metrics:\")\n",
    "print(\"dimensions: \",test_matrix.shape)\n",
    "print(\"Summation value of the utility matrix:\", sum_test_matrix)\n",
    "print(\"Highest row sum of the utility matrix:\", highest_row_sum)\n",
    "print(\"Highest column sum of the utility matrix:\", highest_column_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag-Tag Recommandation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = utility_matrix[:user_cutoff]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Center the data by subtracting the mean of each item (column)\n",
    "# Subtract column means from each value\n",
    "df_centered = df.sub(df.mean(axis=1), axis=0)\n",
    "\n",
    "# Pearson correlation matrix: compute pairwise correlation between items\n",
    "similarity_matrix = df_centered.corr(method='pearson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, item_id, ratings, similarity_matrix, N , type):\n",
    "    \n",
    "    # Get the similarity scores for the target item with all other items\n",
    "    item_similarities = np.array(similarity_matrix[item_id][:tag_cutoff])\n",
    "    \n",
    "    # Get the ratings of user_id for all items\n",
    "    user_ratings = np.array(ratings[user_id])\n",
    "    \n",
    "    # Sort the similarities and take the top N most similar items (excluding the target item itself)\n",
    "    similar_items = np.argsort(item_similarities)[::-1]  # Sort indices by similarity in descending order\n",
    "\n",
    "    start = 0\n",
    "    while start < len(similar_items):\n",
    "        if np.isnan(item_similarities[similar_items[start]]): start += 1\n",
    "        else : break\n",
    "    \n",
    "    non_nan_similar_items = similar_items[start:]\n",
    "\n",
    "    # Filter out items the user hasn't rated\n",
    "    rated_items = [item for item in non_nan_similar_items \n",
    "                         if not np.isnan(ratings[user_id][item])]\n",
    "    \n",
    "    top_n_rated_items = rated_items[:N]  # Select top N similar items\n",
    "\n",
    "\n",
    "    if len(top_n_rated_items) < N:\n",
    "        return np.nanmean(user_ratings)\n",
    "    \n",
    "    # Get the user's ratings for the top N similar items and the corresponding similarities\n",
    "    top_n_ratings = user_ratings[top_n_rated_items]\n",
    "    top_n_similarities = item_similarities[top_n_rated_items]\n",
    "    \n",
    "    if type == \"A\":\n",
    "     return np.sum(top_n_ratings) / N\n",
    "\n",
    "    # Calculate the weighted sum of the ratings\n",
    "    weighted_ratings_sum = np.dot(top_n_ratings, top_n_similarities)\n",
    "    \n",
    "    # Calculate the sum of the absolute values of the similarities\n",
    "\n",
    "    similarity_sum = np.sum(top_n_similarities)\n",
    "    \n",
    "    # Return the weighted average as the predicted rating\n",
    "    if similarity_sum != 0 : \n",
    "        predicted_rating = weighted_ratings_sum / similarity_sum\n",
    "    else : \n",
    "        print(\"1\")\n",
    "        predicted_rating = 0\n",
    "    \n",
    "    return predicted_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with k=  2 : 0.6714782216926365\n",
      "Loss with k=  3 : 0.6519719714508565\n",
      "Loss with k=  5 : 0.6257886856029714\n"
     ]
    }
   ],
   "source": [
    "for k in {2,3,5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x in range(user_cutoff,num_users):\n",
    "        for i in range(tag_cutoff,num_tags):\n",
    "            if np.isnan(utility_matrix[x][i]): continue\n",
    "            else : \n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x,i,utility_matrix,similarity_matrix,k,\"A\")\n",
    "                if(np.isnan(ans)) : \n",
    "                    print(x,i,'Hello')\n",
    "                loss = loss + (true_value - ans)*(true_value - ans)\n",
    "                cnt += 1\n",
    "    print(\"Loss with k= \",k,\":\",np.sqrt(loss / cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with k=  2 : 0.6720502670062237\n",
      "Loss with k=  3 : 0.6519297599842564\n",
      "Loss with k=  5 : 0.6257287296044581\n"
     ]
    }
   ],
   "source": [
    "for k in {2,3,5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x in range(user_cutoff,num_users):\n",
    "        for i in range(tag_cutoff,num_tags):\n",
    "            if np.isnan(utility_matrix[x][i]): continue\n",
    "            else : \n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x,i,utility_matrix,similarity_matrix,k,\"B\")\n",
    "                if(np.isnan(ans)) : \n",
    "                    print(x,i,'Hello')\n",
    "                loss = loss + (true_value - ans)*(true_value - ans)\n",
    "                cnt += 1\n",
    "    print(\"Loss with k= \",k,\":\",np.sqrt(loss / cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-User Recommandation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1163, 827)\n",
      "(1163, 1163)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = utility_matrix[:, :tag_cutoff]\n",
    "print(data.shape)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Center the data by subtracting the mean of each item (column)\n",
    "# Subtract column means from each value\n",
    "df_centered = df.sub(df.mean(axis=0), axis=1)\n",
    "\n",
    "# Pearson correlation matrix: compute pairwise correlation between items\n",
    "similarity_matrix = df_centered.T.corr(method='pearson')\n",
    "print(similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, item_id, ratings, similarity_matrix, N, type, user_cutoff):\n",
    "    # Get the similarity scores for the target user with all other users within the user_cutoff\n",
    "    user_similarities = np.array(similarity_matrix[user_id][:user_cutoff])\n",
    "\n",
    "    # Get the ratings for the item by all users within the user_cutoff\n",
    "    item_ratings = np.array(ratings[:,item_id])\n",
    "\n",
    "    # Sort the similarities and take the top N most similar users (excluding the target user itself)\n",
    "    similar_users = np.argsort(user_similarities)[::-1]  # Sort indices by similarity in descending order\n",
    "\n",
    "    # Exclude the target user from the similar users list (if present)\n",
    "    similar_users = similar_users[similar_users != user_id]\n",
    "\n",
    "    start = 0\n",
    "    while start < len(similar_users):\n",
    "        if np.isnan(user_similarities[similar_users[start]]):\n",
    "            start += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    non_nan_similar_users = similar_users[start:]\n",
    "\n",
    "\n",
    "    # Filter out users who haven't rated the item\n",
    "    rated_users = [user for user in non_nan_similar_users if not np.isnan(ratings[user, item_id])]\n",
    "\n",
    "    top_n_rated_users = rated_users[:N]  # Select top N similar users\n",
    "\n",
    "    if len(top_n_rated_users) < N:\n",
    "        print(\"(tag_id:\",item_id,\")\",\"Less similar users for N: \",N,\" \")\n",
    "        return np.nanmean(item_ratings)\n",
    "\n",
    "    # Get the ratings from the top N similar users and the corresponding similarities\n",
    "    top_n_ratings = item_ratings[top_n_rated_users]\n",
    "    top_n_similarities = user_similarities[top_n_rated_users]\n",
    "\n",
    "    if type == \"A\":\n",
    "        return np.sum(top_n_ratings) / N\n",
    "\n",
    "    # Calculate the weighted sum of the ratings\n",
    "    weighted_ratings_sum = np.dot(top_n_ratings, top_n_similarities)\n",
    "\n",
    "    # Calculate the sum of the absolute values of the similarities\n",
    "    similarity_sum = np.sum(top_n_similarities)\n",
    "\n",
    "    # Return the weighted average as the predicted rating\n",
    "    if similarity_sum != 0:\n",
    "        predicted_rating = weighted_ratings_sum / similarity_sum\n",
    "    else:\n",
    "        print(\"1\")\n",
    "        predicted_rating = 0\n",
    "\n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with k = 2: 0.6078950763421757\n",
      "Loss with k = 3: 0.5806122807470818\n",
      "(tag_id: 862 ) Less similar users for N:  5  \n",
      "(tag_id: 862 ) Less similar users for N:  5  \n",
      "(tag_id: 862 ) Less similar users for N:  5  \n",
      "(tag_id: 862 ) Less similar users for N:  5  \n",
      "(tag_id: 862 ) Less similar users for N:  5  \n",
      "(tag_id: 862 ) Less similar users for N:  5  \n",
      "Loss with k = 5: 0.5671975178741177\n"
     ]
    }
   ],
   "source": [
    "for k in {2, 3, 5}:  # Different values of N (number of similar users to consider)\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for i in range(tag_cutoff, num_tags):  # Iterate through items\n",
    "     for x in range(user_cutoff, num_users):  # Iterate through users\n",
    "            if np.isnan(utility_matrix[x][i]):  # Skip if the user hasn't rated the item\n",
    "                continue\n",
    "            else:\n",
    "                true_value = utility_matrix[x][i]  # Actual rating\n",
    "                ans = predict_rating(x, i, utility_matrix, similarity_matrix, k, \"A\", user_cutoff)  # Predict rating\n",
    "                \n",
    "                if np.isnan(ans): \n",
    "                    print(x, i, 'Hello')\n",
    "                    continue  # Skip NaN predictions to avoid errors\n",
    "                \n",
    "                # Calculate squared error\n",
    "                loss += (true_value - ans) ** 2\n",
    "                cnt += 1\n",
    "    \n",
    "    # Calculate and print RMSE (Root Mean Squared Error) for the current value of k\n",
    "    print(f\"Loss with k = {k}: {np.sqrt(loss / cnt)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with k = 2: 0.607478275445503\n",
      "Loss with k = 3: 0.5801996120223132\n",
      "(tag_id: 862 ) Less similar users for N:  5  \n",
      "(tag_id: 862 ) Less similar users for N:  5  \n",
      "(tag_id: 862 ) Less similar users for N:  5  \n",
      "(tag_id: 862 ) Less similar users for N:  5  \n",
      "(tag_id: 862 ) Less similar users for N:  5  \n",
      "(tag_id: 862 ) Less similar users for N:  5  \n",
      "Loss with k = 5: 0.7369209013096282\n"
     ]
    }
   ],
   "source": [
    "for k in {2, 3, 5}:  # Different values of N (number of similar users to consider)\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for i in range(tag_cutoff, num_tags):  # Iterate through items\n",
    "     for x in range(user_cutoff, num_users):  # Iterate through users\n",
    "            if np.isnan(utility_matrix[x][i]):  # Skip if the user hasn't rated the item\n",
    "                continue\n",
    "            else:\n",
    "                true_value = utility_matrix[x][i]  # Actual rating\n",
    "                ans = predict_rating(x, i, utility_matrix, similarity_matrix, k, \"B\", user_cutoff)  # Predict rating\n",
    "                \n",
    "                if np.isnan(ans): \n",
    "                    print(x, i, 'Hello')\n",
    "                    continue  # Skip NaN predictions to avoid errors\n",
    "                \n",
    "                # Calculate squared error\n",
    "                loss += (true_value - ans) ** 2\n",
    "                cnt += 1\n",
    "    \n",
    "    # Calculate and print RMSE (Root Mean Squared Error) for the current value of k\n",
    "    print(f\"Loss with k = {k}: {np.sqrt(loss / cnt)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### Question 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMatrixFactorizationSGD\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, R, K, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, lambda1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, lambda2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class MatrixFactorizationSGD:\n",
    "    def __init__(self, R, K, alpha=0.0005, epochs=10, lambda1=0, lambda2=0):\n",
    "        \"\"\"\n",
    "        Initialize the matrix factorization model with two regularization parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        R : np.array (users x items) -> Utility matrix with values NaN, 0, 1, 2, 3, 4, 5\n",
    "        K : int -> Number of latent factors\n",
    "        alpha : float -> Learning rate\n",
    "        epochs : int -> Number of full passes through the data\n",
    "        lambda1 : float -> Regularization parameter for user-feature matrix P\n",
    "        lambda2 : float -> Regularization parameter for item-feature matrix Q\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K  # Number of latent factors\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.epochs = epochs  # Number of full passes (epochs)\n",
    "        self.lambda1 = lambda1  # Regularization parameter for P\n",
    "        self.lambda2 = lambda2  # Regularization parameter for Q\n",
    "        self.P = np.random.rand(self.num_users, K)  # User-feature matrix\n",
    "        self.Q = np.random.rand(self.num_items, K)  # Item-feature matrix\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the matrix factorization model using stochastic gradient descent (SGD) over multiple epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(self.num_users):\n",
    "                for j in range(self.num_items):\n",
    "                    if not np.isnan(self.R[i][j]):  # Skip NaN values\n",
    "                        # Compute the prediction error\n",
    "                        eij = self.R[i][j] - np.dot(self.P[i, :], self.Q[j, :])\n",
    "\n",
    "                        # Update P and Q matrices with regularization terms lambda1 and lambda2\n",
    "                        for k in range(self.K):\n",
    "                            self.P[i][k] += self.alpha * (2 * eij * self.Q[j][k] - 2 * self.lambda1 * self.P[i][k])\n",
    "                            self.Q[j][k] += self.alpha * (2 * eij * self.P[i][k] - 2 * self.lambda2 * self.Q[j][k])\n",
    "            \n",
    "            # Compute and print the loss at the end of each epoch\n",
    "            loss = self.compute_loss()\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}: Loss = {loss}\")\n",
    "        \n",
    "        return self.P, self.Q\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \"\"\"\n",
    "        Compute the cost function (loss) with two regularization parameters.\n",
    "        The loss includes the squared error and regularization terms.\n",
    "        \"\"\"\n",
    "        predicted_R = np.dot(self.P, self.Q.T)\n",
    "        loss = 0\n",
    "        num_non_nan_entries = 0\n",
    "\n",
    "        for i in range(self.num_users):\n",
    "            for j in range(self.num_items):\n",
    "                if not np.isnan(self.R[i][j]):  # Only consider non-NaN values\n",
    "                    error = self.R[i][j] - predicted_R[i][j]\n",
    "                    loss += error ** 2\n",
    "                    num_non_nan_entries += 1\n",
    "        \n",
    "        # Adding regularization terms for P and Q matrices\n",
    "        loss += self.lambda1 * np.sum(self.P ** 2)  # Regularization for P\n",
    "        loss += self.lambda2 * np.sum(self.Q ** 2)  # Regularization for Q\n",
    "\n",
    "        # Normalize the loss by the number of non-NaN entries\n",
    "        loss /= num_non_nan_entries\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predict the rating matrix after training.\n",
    "        \"\"\"\n",
    "        return np.dot(self.P, self.Q.T)\n",
    "\n",
    "    def rmse(self, predicted_R):\n",
    "        \"\"\"\n",
    "        Compute the Root Mean Square Error (RMSE) between the predicted and actual ratings.\n",
    "        \"\"\"\n",
    "        actual_ratings = self.R[~np.isnan(self.R)]\n",
    "        predicted_ratings = predicted_R[~np.isnan(self.R)]\n",
    "        return np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "\n",
    "# Test the updated modular code with a sample utility matrix\n",
    "def run_tests():\n",
    "    # Sample utility matrix (NaN represents missing ratings)\n",
    "    R = utility_matrix\n",
    "\n",
    "    # Parameters\n",
    "    latent_factors = [2, 5, 10]\n",
    "    alpha = 0.0005\n",
    "    epochs = 10\n",
    "    lambda1List = [0, 0.001, 0.05, 0.5]  # Regularization parameter for P\n",
    "    lambda2List = [0, 0.003, 0.05, 0.75]  # Regularization parameter for Q\n",
    "\n",
    "    for K in latent_factors:\n",
    "        for lambda1, lambda2 in zip(lambda1List, lambda2List):\n",
    "            mf_reg = MatrixFactorizationSGD(R, K, alpha, epochs, lambda1=lambda1, lambda2=lambda2)\n",
    "            P_reg, Q_reg = mf_reg.train()\n",
    "            predicted_R_reg = mf_reg.predict()\n",
    "            rmse_reg = mf_reg.rmse(predicted_R_reg)\n",
    "\n",
    "            print(f\"Latent Factors: {K}\")\n",
    "            print(f\"RMSE with regularization (lambda1={lambda1}, lambda2={lambda2}): {rmse_reg}\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
