{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment1: Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "posts_df = pd.read_csv('./csv/Posts.csv')\n",
    "Tags = pd.read_csv('./csv/Tags.csv')\n",
    "\n",
    "answers_df = posts_df[posts_df['PostTypeId'] == 2][['Id', 'OwnerUserId', 'ParentId']]\n",
    "answers_df = answers_df.drop_duplicates(subset=['OwnerUserId','ParentId'])\n",
    "answers_df['ParentId'] = answers_df['ParentId'].astype(int)\n",
    "\n",
    "questions_df = posts_df[posts_df['PostTypeId'] == 1][['Id', 'Tags']]\n",
    "\n",
    "questions_df['Id'] = questions_df['Id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 users with the most answers:\n",
      "       OwnerUserId  AnswerCount\n",
      "3189        9113.0         2838\n",
      "19912     177980.0         2318\n",
      "557         1204.0         2042\n",
      "\n",
      "Top 3 most used tags:\n",
      "    TagName  Count\n",
      "259  design   5162\n",
      "114      c#   4931\n",
      "37     java   4929\n"
     ]
    }
   ],
   "source": [
    "answerers_table = answers_df.groupby('OwnerUserId').size().reset_index(name='AnswerCount')\n",
    "top_answerers = answerers_table.sort_values(by='AnswerCount', ascending=False).head(3)\n",
    "\n",
    "top_tags = Tags[['TagName', 'Count']].sort_values(by='Count', ascending=False).head(3)\n",
    "\n",
    "print(\"Top 3 users with the most answers:\")\n",
    "print(top_answerers)\n",
    "\n",
    "print(\"\\nTop 3 most used tags:\")\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1 : First attach corresponding tags for answers by table join with questions table using Parent Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Qualified Answerers: 1160\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(answers_df, questions_df, left_on='ParentId', right_on='Id', suffixes=('_answer', '_question'))\n",
    "\n",
    "filtered_answers_df = merged_df[['Id_answer', 'OwnerUserId', 'Tags','ParentId']] \n",
    "filtered_answers_df = filtered_answers_df.drop_duplicates(['OwnerUserId','ParentId'])\n",
    "answerer_counts = filtered_answers_df.groupby('OwnerUserId').size()\n",
    "qualified_answerers = answerer_counts[answerer_counts >= 20].index\n",
    "print(\"No. of Qualified Answerers:\", len(qualified_answerers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step2 : Filter the answers using the qualified answers ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered answerers:    Id_answer  OwnerUserId                                               Tags  \\\n",
      "0          3         11.0                           |comments|anti-patterns|   \n",
      "3         13          4.0                           |comments|anti-patterns|   \n",
      "4         20          6.0                     |productivity|time-management|   \n",
      "6         23         11.0                     |productivity|time-management|   \n",
      "8         26         17.0  |business|project-management|development-process|   \n",
      "\n",
      "   ParentId  \n",
      "0         1  \n",
      "3         1  \n",
      "4         9  \n",
      "6         9  \n",
      "8         4  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_answers = filtered_answers_df[filtered_answers_df['OwnerUserId'].isin(qualified_answerers)]\n",
    "print(\"filtered answerers:\" , filtered_answers.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step3 :  Filter tags and expand the answers tables by expanding rows for each tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualified Tags: 974\n"
     ]
    }
   ],
   "source": [
    "qualified_tags = Tags[Tags['Count'] >= 20]['Id']\n",
    "print(\"Qualified Tags:\", len(qualified_tags))\n",
    "\n",
    "tag_dict = Tags.set_index('TagName')['Id'].to_dict()\n",
    "\n",
    "tags_expanded = filtered_answers.copy()\n",
    "\n",
    "tags_expanded['Tags'] = tags_expanded['Tags'].str.split('|').apply(lambda x: x[1:-1])\n",
    "tags_expanded = tags_expanded.explode('Tags')\n",
    "tags_expanded['Tags'] = tags_expanded['Tags'].map(tag_dict)\n",
    "tags_expanded = tags_expanded[tags_expanded['Tags'].isin(qualified_tags)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step4 : Create Utility matrix from the filtered answers table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert Matrix: Tags         1     3     4     7     8     9     11    12    13    14    ...  \\\n",
      "OwnerUserId                                                              ...   \n",
      "4            13.0   NaN   6.0   6.0  61.0  55.0   8.0   3.0   NaN   NaN  ...   \n",
      "6             NaN   NaN   8.0   NaN   6.0   4.0   1.0   2.0   NaN   NaN  ...   \n",
      "11            1.0   NaN   1.0   NaN   NaN   1.0   NaN   1.0   NaN   NaN  ...   \n",
      "14            NaN   NaN   1.0   NaN   1.0   1.0   NaN   1.0   NaN   NaN  ...   \n",
      "15            1.0   NaN   2.0   1.0   4.0   4.0   1.0   1.0   NaN   NaN  ...   \n",
      "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
      "356695        NaN   NaN   NaN   NaN   NaN   1.0   NaN   NaN   NaN   NaN  ...   \n",
      "366014        NaN   NaN   NaN   NaN   NaN   NaN   1.0   NaN   NaN   NaN  ...   \n",
      "373864        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "378329        1.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "379622        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "\n",
      "Tags         4639  4646  4661  4682  4683  4687  4690  4704  4720  4750  \n",
      "OwnerUserId                                                              \n",
      "4             NaN   NaN   NaN   2.0   1.0   1.0   NaN   NaN   1.0   NaN  \n",
      "6             NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "11            NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "14            NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "15            NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "356695        NaN   NaN   1.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "366014        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "373864        NaN   NaN   2.0   NaN   NaN   NaN   1.0   NaN   NaN   NaN  \n",
      "378329        NaN   NaN   1.0   NaN   NaN   NaN   NaN   NaN   5.0   1.0  \n",
      "379622        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "\n",
      "[1160 rows x 973 columns]\n",
      "Dimensions of the Expert matrix: (1160, 973)\n"
     ]
    }
   ],
   "source": [
    "expert_matrix_df = pd.pivot_table(\n",
    "    tags_expanded, \n",
    "    index='OwnerUserId', \n",
    "    columns='Tags', \n",
    "    aggfunc='size', \n",
    "    fill_value=np.nan\n",
    ")\n",
    "\n",
    "expert_matrix_df.index = expert_matrix_df.index.astype(int)\n",
    "expert_matrix_df.columns = expert_matrix_df.columns.astype(int)\n",
    "\n",
    "all_qualified_tags = pd.Series(qualified_tags, name='Tags')\n",
    "print(\"Expert Matrix:\", expert_matrix_df)\n",
    "print(\"Dimensions of the Expert matrix:\", expert_matrix_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step5: convert to numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13. nan  6. ... nan  1. nan]\n",
      " [nan nan  8. ... nan nan nan]\n",
      " [ 1. nan  1. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 1. nan nan ... nan  5.  1.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "expert_matrix = expert_matrix_df.to_numpy()\n",
    "print(expert_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Normalize the utility matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "normalize = lambda x: np.nan if np.isnan(x) else (np.floor(x / 3) if x < 15 else 5)\n",
    "vectorized_modify_entries = np.vectorize(normalize)\n",
    "utility_matrix = vectorized_modify_entries(expert_matrix)\n",
    "utility_matrix_df = expert_matrix_df.map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4. nan  2. ... nan  0. nan]\n",
      " [nan nan  2. ... nan nan nan]\n",
      " [ 0. nan  0. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 0. nan nan ... nan  1.  0.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(utility_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility Matrix Metrics:\n",
      "Summation value of the utility matrix: 41180.0\n",
      "Highest row sum of the utility matrix: 1162.0\n",
      "Highest column sum of the utility matrix: 1403.0\n"
     ]
    }
   ],
   "source": [
    "sum_utility_matrix = np.nansum(utility_matrix)\n",
    "highest_row_sum = np.max(np.nansum(utility_matrix, axis=1))\n",
    "highest_column_sum = np.max(np.nansum(utility_matrix, axis=0))\n",
    "\n",
    "print(\"Utility Matrix Metrics:\")\n",
    "print(\"Summation value of the utility matrix:\", sum_utility_matrix)\n",
    "print(\"Highest row sum of the utility matrix:\", highest_row_sum)\n",
    "print(\"Highest column sum of the utility matrix:\", highest_column_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Create Train and Test matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test Matrix Metrics:\n",
      "Summation value of the train matrix: 40538.0\n",
      "Dimension of Test Matrix: (174, 146)\n",
      "Summation value of the test matrix: 642.0\n"
     ]
    }
   ],
   "source": [
    "num_users, num_tags = utility_matrix.shape\n",
    "user_cutoff = int(num_users * 0.85)\n",
    "tag_cutoff = int(num_tags * 0.85)\n",
    "\n",
    "train_matrix = utility_matrix.copy()\n",
    "train_df = utility_matrix_df.copy()\n",
    "\n",
    "\n",
    "train_matrix[user_cutoff:, tag_cutoff:] = np.nan\n",
    "train_df.iloc[user_cutoff: ,tag_cutoff:] = np.nan\n",
    "train_matrix_sum = np.nansum(train_matrix)\n",
    "\n",
    "test_matrix = utility_matrix[user_cutoff:, tag_cutoff:]\n",
    "test_df = utility_matrix_df.iloc[user_cutoff: , tag_cutoff:]\n",
    "sum_test_matrix = np.nansum(test_matrix)\n",
    "\n",
    "print(\"Train and Test Matrix Metrics:\")\n",
    "print(\"Summation value of the train matrix:\", train_matrix_sum)\n",
    "print(\"Dimension of Test Matrix:\",test_matrix.shape)\n",
    "print(\"Summation value of the test matrix:\", sum_test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag-Tag Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-1: Calculate Similarity matrix for Tag-Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = utility_matrix[:user_cutoff]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_centered = df.sub(df.mean(axis=1), axis=0)\n",
    "\n",
    "similarity_matrix = df_centered.corr(method='pearson')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-2: Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, item_id, ratings, similarity_matrix, N , type):\n",
    "    \n",
    "    item_similarities = np.array(similarity_matrix[item_id][:tag_cutoff])\n",
    "    user_ratings = np.array(ratings[user_id])\n",
    "    similar_items = np.argsort(item_similarities)[::-1]\n",
    "\n",
    "    start = 0\n",
    "    while start < len(similar_items):\n",
    "        if np.isnan(item_similarities[similar_items[start]]): start += 1\n",
    "        else : break\n",
    "    \n",
    "    non_nan_similar_items = similar_items[start:]\n",
    "\n",
    "    rated_items = [item for item in non_nan_similar_items \n",
    "                         if not np.isnan(ratings[user_id][item])]\n",
    "    top_n_rated_items = rated_items[:N]  \n",
    "\n",
    "    if len(top_n_rated_items) < N:\n",
    "        return np.nanmean(user_ratings)\n",
    "    \n",
    "    top_n_ratings = user_ratings[top_n_rated_items]\n",
    "    top_n_similarities = item_similarities[top_n_rated_items]\n",
    "    \n",
    "    if type == \"A\":\n",
    "     return np.sum(top_n_ratings) / N\n",
    "\n",
    "    weighted_ratings_sum = np.dot(top_n_ratings, top_n_similarities)\n",
    "\n",
    "    similarity_sum = np.sum(top_n_similarities)\n",
    "    if similarity_sum != 0 : \n",
    "        predicted_rating = weighted_ratings_sum / similarity_sum\n",
    "    else : \n",
    "        predicted_rating = 0\n",
    "    \n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-3: Predict and Calculate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Average Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Average\n",
      "Loss with k = 2 : 0.8368331915377201\n",
      "Loss with k = 3 : 0.8068537836476033\n",
      "Loss with k = 5 : 0.7667057566908195\n"
     ]
    }
   ],
   "source": [
    "print(\"Simple Average\")\n",
    "for k in {2,3,5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x in range(user_cutoff,num_users):\n",
    "        for i in range(tag_cutoff,num_tags):\n",
    "            if np.isnan(utility_matrix[x][i]): continue\n",
    "            else : \n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x,i,utility_matrix,similarity_matrix,k,\"A\")\n",
    "                if(np.isnan(ans)) : \n",
    "                    print(x,i,'Hello')\n",
    "                loss = loss + (true_value - ans)*(true_value - ans)\n",
    "                cnt += 1\n",
    "    print(\"Loss with k =\",k,\":\",np.sqrt(loss / cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weighted Average Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average\n",
      "Loss with k = 2 : 0.8368997316823331\n",
      "Loss with k = 3 : 0.8066557194070604\n",
      "Loss with k = 5 : 0.768163144485662\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted Average\")\n",
    "for k in {2,3,5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x in range(user_cutoff,num_users):\n",
    "        for i in range(tag_cutoff,num_tags):\n",
    "            if np.isnan(utility_matrix[x][i]): continue\n",
    "            else : \n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x,i,utility_matrix,similarity_matrix,k,\"B\")\n",
    "                if(np.isnan(ans)) : \n",
    "                    print(x,i,'Hello')\n",
    "                loss = loss + (true_value - ans)*(true_value - ans)\n",
    "                cnt += 1\n",
    "    print(\"Loss with k =\",k,\":\",np.sqrt(loss / cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-User Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = utility_matrix[:, :tag_cutoff]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df_centered = df.sub(df.mean(axis=0), axis=1)\n",
    "\n",
    "similarity_matrix = df_centered.T.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Function with Simple Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, item_id, ratings, similarity_matrix, N, type, user_cutoff):\n",
    "    user_similarities = np.array(similarity_matrix[user_id][:user_cutoff])\n",
    "    item_ratings = np.array(ratings[:,item_id])\n",
    "    similar_users = np.argsort(user_similarities)[::-1]\n",
    "    similar_users = similar_users[similar_users != user_id]\n",
    "\n",
    "    start = 0\n",
    "    while start < len(similar_users):\n",
    "        if np.isnan(user_similarities[similar_users[start]]):\n",
    "            start += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    non_nan_similar_users = similar_users[start:]\n",
    "\n",
    "    rated_users = [user for user in non_nan_similar_users if not np.isnan(ratings[user, item_id])]\n",
    "\n",
    "    top_n_rated_users = rated_users[:N]\n",
    "\n",
    "    if len(top_n_rated_users) < N:\n",
    "        return np.nanmean(item_ratings)\n",
    "\n",
    "    top_n_ratings = item_ratings[top_n_rated_users]\n",
    "    top_n_similarities = user_similarities[top_n_rated_users]\n",
    "\n",
    "    if type == \"A\":\n",
    "        return np.sum(top_n_ratings)/N\n",
    "\n",
    "    weighted_ratings_sum = np.dot(top_n_ratings, top_n_similarities)\n",
    "\n",
    "    similarity_sum = np.sum(top_n_similarities)\n",
    "\n",
    "    if similarity_sum != 0:\n",
    "        predicted_rating = weighted_ratings_sum / similarity_sum\n",
    "    else:\n",
    "        predicted_rating = 0\n",
    "\n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Average\n",
      "Loss with k = 2: 0.7006938793917289,2243\n",
      "Loss with k = 3: 0.690393644492957,2243\n",
      "Loss with k = 5: 0.6769631382163931,2243\n"
     ]
    }
   ],
   "source": [
    "print(\"Simple Average\")\n",
    "for k in {2, 3, 5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for i in range(tag_cutoff, num_tags):\n",
    "     for x in range(user_cutoff, num_users):\n",
    "            if np.isnan(utility_matrix[x][i]):\n",
    "                continue\n",
    "            else:\n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x, i, utility_matrix, similarity_matrix, k, \"A\", user_cutoff)\n",
    "                \n",
    "                if np.isnan(ans): \n",
    "                    print(x, i, 'Hello')\n",
    "                    continue\n",
    "                \n",
    "                loss += (true_value - ans) ** 2\n",
    "                cnt += 1\n",
    "    \n",
    "    print(f\"Loss with k = {k}: {np.sqrt(loss / cnt)},{cnt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average\n",
      "Loss with k = 2: 1.0257074115306568\n",
      "Loss with k = 3: 0.7457370435666525\n",
      "Loss with k = 5: 0.6830546662035217\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted Average\")\n",
    "for k in {2, 3, 5}: \n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for i in range(tag_cutoff, num_tags):\n",
    "     for x in range(user_cutoff, num_users):  \n",
    "            if np.isnan(utility_matrix[x][i]):\n",
    "                continue\n",
    "            else:\n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x, i, utility_matrix, similarity_matrix, k, \"B\", user_cutoff)\n",
    "                \n",
    "                if np.isnan(ans): \n",
    "                    continue\n",
    "                \n",
    "                loss += (true_value - ans) ** 2\n",
    "                cnt += 1\n",
    "\n",
    "    print(f\"Loss with k = {k}: {np.sqrt(loss / cnt)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### Question 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class MatrixFactorizationSGD:\n",
    "    def __init__(self, R, K, alpha=0.0005, epochs=10, lambda1=0, lambda2=0):\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K \n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.P = np.random.normal(scale=1./K, size=(self.num_users, K))\n",
    "        self.Q = np.random.normal(scale=1./K, size=(self.num_items, K))\n",
    "        # self.P = np.random.rand(self.num_users, K)  # User-feature matrix\n",
    "        # self.Q = np.random.rand(self.num_items, K)  # Item-feature matrix\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(self.num_users):\n",
    "                for j in range(self.num_items):\n",
    "                    if not np.isnan(self.R[i][j]):\n",
    "                        eij = self.R[i][j] - np.dot(self.P[i, :], self.Q[j, :])\n",
    "                        self.P[i][:] += self.alpha * (2 * eij * self.Q[j][:] - 2 * self.lambda1 * self.P[i][:])\n",
    "                        self.Q[j][:] += self.alpha * (2 * eij * self.P[i][:] - 2 * self.lambda2 * self.Q[j][:])\n",
    "            \n",
    "            loss = self.compute_loss()\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}: Loss = {loss}\")\n",
    "        \n",
    "        return self.P, self.Q\n",
    "\n",
    "    def compute_loss(self):\n",
    "        predicted_R = np.dot(self.P, self.Q.T)\n",
    "        loss = 0\n",
    "        num_non_nan_entries = 0\n",
    "\n",
    "        for i in range(self.num_users):\n",
    "            for j in range(self.num_items):\n",
    "                if not np.isnan(self.R[i][j]):\n",
    "                    error = self.R[i][j] - predicted_R[i][j]\n",
    "                    loss += error ** 2\n",
    "                    num_non_nan_entries += 1\n",
    "        \n",
    "        loss += self.lambda1 * np.sum(self.P ** 2)\n",
    "        loss += self.lambda2 * np.sum(self.Q ** 2)\n",
    "        loss /= num_non_nan_entries\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict(self, test_matrix):\n",
    "        predicted_R = np.dot(self.P, self.Q.T)\n",
    "        predicted_R_test = np.copy(test_matrix)\n",
    "        \n",
    "        for i in range(test_matrix.shape[0]):\n",
    "            for j in range(test_matrix.shape[1]):\n",
    "                if test_matrix[i][j] is not None:\n",
    "                    predicted_R_test[i][j] = predicted_R[i+user_cutoff][j+tag_cutoff]\n",
    "\n",
    "        return predicted_R_test\n",
    "    \n",
    "    def rmse(self, predicted_R, matrix):\n",
    "        mask = ~np.isnan(matrix)\n",
    "        actual_ratings = matrix[mask]\n",
    "        predicted_ratings = predicted_R[mask]\n",
    "        return np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Loss = 1.016373251602643\n",
      "Epoch 2/50: Loss = 0.9959827107035324\n",
      "Epoch 3/50: Loss = 0.9812743307650574\n",
      "Epoch 4/50: Loss = 0.9699504081844296\n",
      "Epoch 5/50: Loss = 0.9606303630029334\n",
      "Epoch 6/50: Loss = 0.9523923222336005\n",
      "Epoch 7/50: Loss = 0.9445408949322069\n",
      "Epoch 8/50: Loss = 0.9364774751183278\n",
      "Epoch 9/50: Loss = 0.9276209662909884\n",
      "Epoch 10/50: Loss = 0.9173573772682662\n",
      "Epoch 11/50: Loss = 0.9050124956682538\n",
      "Epoch 12/50: Loss = 0.8898523059464707\n",
      "Epoch 13/50: Loss = 0.8711237273971936\n",
      "Epoch 14/50: Loss = 0.8481517474830981\n",
      "Epoch 15/50: Loss = 0.8205017214663773\n",
      "Epoch 16/50: Loss = 0.7881888500942859\n",
      "Epoch 17/50: Loss = 0.7518694301422807\n",
      "Epoch 18/50: Loss = 0.7129035251721156\n",
      "Epoch 19/50: Loss = 0.67318818153183\n",
      "Epoch 20/50: Loss = 0.6347635706001945\n",
      "Epoch 21/50: Loss = 0.5993447499986889\n",
      "Epoch 22/50: Loss = 0.5679979684180884\n",
      "Epoch 23/50: Loss = 0.5410850464560236\n",
      "Epoch 24/50: Loss = 0.5184283452623974\n",
      "Epoch 25/50: Loss = 0.4995531277747763\n",
      "Epoch 26/50: Loss = 0.48388903743480094\n",
      "Epoch 27/50: Loss = 0.47088884023000555\n",
      "Epoch 28/50: Loss = 0.4600775978888411\n",
      "Epoch 29/50: Loss = 0.45106154447638175\n",
      "Epoch 30/50: Loss = 0.4435198901217121\n",
      "Epoch 31/50: Loss = 0.4371922277082657\n",
      "Epoch 32/50: Loss = 0.4318667046791603\n",
      "Epoch 33/50: Loss = 0.42737033970262295\n",
      "Epoch 34/50: Loss = 0.4235614213309845\n",
      "Epoch 35/50: Loss = 0.4203235614405464\n",
      "Epoch 36/50: Loss = 0.417560992316089\n",
      "Epoch 37/50: Loss = 0.4151947981015609\n",
      "Epoch 38/50: Loss = 0.41315986351179335\n",
      "Epoch 39/50: Loss = 0.4114023871160717\n",
      "Epoch 40/50: Loss = 0.4098778479532773\n",
      "Epoch 41/50: Loss = 0.4085493407864526\n",
      "Epoch 42/50: Loss = 0.4073862130672077\n",
      "Epoch 43/50: Loss = 0.40636294939232365\n",
      "Epoch 44/50: Loss = 0.4054582589505392\n",
      "Epoch 45/50: Loss = 0.4046543292471614\n",
      "Epoch 46/50: Loss = 0.40393621581225114\n",
      "Epoch 47/50: Loss = 0.4032913429473901\n",
      "Epoch 48/50: Loss = 0.4027090950323672\n",
      "Epoch 49/50: Loss = 0.40218048163312253\n",
      "Epoch 50/50: Loss = 0.4016978627326971\n",
      "Latent Factor: 2\n",
      "RMSE on train data with regularization (lambda1=0, lambda2=0): 0.6337963890183461\n",
      "RMSE on test data with regularization (lambda1=0, lambda2=0): 0.7246158252800303\n",
      "Epoch 1/50: Loss = 1.0121513016412615\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lambda1, lambda2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lambda1List, lambda2List):\n\u001b[1;32m     12\u001b[0m     mf_reg \u001b[38;5;241m=\u001b[39m MatrixFactorizationSGD(R_train, K, alpha, epochs, lambda1\u001b[38;5;241m=\u001b[39mlambda1, lambda2\u001b[38;5;241m=\u001b[39mlambda2)\n\u001b[0;32m---> 13\u001b[0m     P_reg, Q_reg \u001b[38;5;241m=\u001b[39m \u001b[43mmf_reg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     predicted_R_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(P_reg, Q_reg\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m     16\u001b[0m     train_rmse \u001b[38;5;241m=\u001b[39m mf_reg\u001b[38;5;241m.\u001b[39mrmse(predicted_R_train, R_train)\n",
      "Cell \u001b[0;32mIn[90], line -1\u001b[0m, in \u001b[0;36mMatrixFactorizationSGD.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "R_train = train_matrix\n",
    "R_test = test_matrix\n",
    "\n",
    "latent_factors = [2, 5, 10]\n",
    "alpha = 0.0005\n",
    "epochs = 50\n",
    "lambda1List = [0, 0.001, 0.05, 0.5]\n",
    "lambda2List = [0, 0.003, 0.05, 0.75]\n",
    "\n",
    "for K in latent_factors:\n",
    "    for lambda1, lambda2 in zip(lambda1List, lambda2List):\n",
    "        mf_reg = MatrixFactorizationSGD(R_train, K, alpha, epochs, lambda1=lambda1, lambda2=lambda2)\n",
    "        P_reg, Q_reg = mf_reg.train()\n",
    "\n",
    "        predicted_R_train = np.dot(P_reg, Q_reg.T)\n",
    "        train_rmse = mf_reg.rmse(predicted_R_train, R_train)\n",
    "\n",
    "        predicted_R_reg = mf_reg.predict(R_test)\n",
    "        rmse_reg = mf_reg.rmse(predicted_R_reg, R_test)\n",
    "\n",
    "        print(f\"Latent Factor: {K}\")\n",
    "        print(f\"RMSE on train data with regularization (lambda1={lambda1}, lambda2={lambda2}): {train_rmse}\")\n",
    "        print(f\"RMSE on test data with regularization (lambda1={lambda1}, lambda2={lambda2}): {rmse_reg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, KNNBaseline\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_surprise_df(matrix):\n",
    "    df = matrix.stack().reset_index()\n",
    "    df.columns = ['user', 'item', 'rating']\n",
    "    return df[pd.notna(df['rating'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surprise_knn(train_matrix, test_matrix, sim_options, N_values):\n",
    "    train_df = matrix_to_surprise_df(train_matrix)\n",
    "    print(train_df)\n",
    "    test_df = matrix_to_surprise_df(test_matrix)\n",
    "    print(test_df)\n",
    "    \n",
    "    reader = Reader(rating_scale=(0, max(train_matrix.max().max(), test_matrix.max().max())))\n",
    "    train_data = Dataset.load_from_df(train_df[['user', 'item', 'rating']], reader)\n",
    "    trainset = train_data.build_full_trainset()\n",
    "    testset = list(test_df.itertuples(index=False, name=None))\n",
    "\n",
    "    rmse_results = {}\n",
    "    \n",
    "    for N in N_values:\n",
    "        sim_options['k'] = N\n",
    "        algo = KNNBaseline(k=N, sim_options=sim_options)\n",
    "        algo.fit(trainset)\n",
    "        predictions = algo.test(testset)\n",
    "        rmse = accuracy.rmse(predictions, verbose=False)\n",
    "        rmse_results[N] = rmse\n",
    "\n",
    "    return rmse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options_item_item = {'name': 'pearson_baseline', 'user_based': False}  # Item-Item\n",
    "sim_options_user_user = {'name': 'pearson_baseline', 'user_based': True}   # User-User\n",
    "N_values = [2, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          user  item  rating\n",
      "0            4     1     4.0\n",
      "1            4     4     2.0\n",
      "2            4     7     2.0\n",
      "3            4     8     5.0\n",
      "4            4     9     5.0\n",
      "...        ...   ...     ...\n",
      "118222  379622  2845     0.0\n",
      "118223  379622  2876     0.0\n",
      "118224  379622  3018     0.0\n",
      "118225  379622  3199     0.0\n",
      "118226  379622  3242     0.0\n",
      "\n",
      "[118227 rows x 3 columns]\n",
      "        user  item  rating\n",
      "0     121035  3393     0.0\n",
      "1     121035  3586     0.0\n",
      "2     121035  3631     0.0\n",
      "3     121035  3757     0.0\n",
      "4     121035  3761     0.0\n",
      "...      ...   ...     ...\n",
      "2238  379622  4094     0.0\n",
      "2239  379622  4095     0.0\n",
      "2240  379622  4164     0.0\n",
      "2241  379622  4206     0.0\n",
      "2242  379622  4533     0.0\n",
      "\n",
      "[2243 rows x 3 columns]\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "rmse_item_item_surprise = surprise_knn(train_df, test_df, sim_options_item_item, N_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surprise Library RMSE Results (Tag-Tag):\n",
      "N=2: RMSE=0.7669281403985956\n",
      "N=3: RMSE=0.7275695777976227\n",
      "N=5: RMSE=0.694168988415908\n"
     ]
    }
   ],
   "source": [
    "print(\"Surprise Library RMSE Results (Tag-Tag):\")\n",
    "for N in rmse_item_item_surprise:\n",
    "    print(f\"N={N}: RMSE={rmse_item_item_surprise[N]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
