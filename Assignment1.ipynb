{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment1: Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "posts_df = pd.read_csv('./csv/Posts.csv')\n",
    "Tags = pd.read_csv('./csv/Tags.csv')\n",
    "\n",
    "answers_df = posts_df[posts_df['PostTypeId'] == 2][['Id', 'OwnerUserId', 'ParentId']]\n",
    "answers_df = answers_df.drop_duplicates(subset=['OwnerUserId','ParentId'])\n",
    "answers_df['ParentId'] = answers_df['ParentId'].astype(int)\n",
    "\n",
    "questions_df = posts_df[posts_df['PostTypeId'] == 1][['Id', 'Tags']]\n",
    "\n",
    "questions_df['Id'] = questions_df['Id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 users with the most answers:\n",
      "       OwnerUserId  AnswerCount\n",
      "3189        9113.0         2838\n",
      "19912     177980.0         2318\n",
      "557         1204.0         2042\n",
      "\n",
      "Top 3 most used tags:\n",
      "    TagName  Count\n",
      "259  design   5162\n",
      "114      c#   4931\n",
      "37     java   4929\n"
     ]
    }
   ],
   "source": [
    "answerers_table = answers_df.groupby('OwnerUserId').size().reset_index(name='AnswerCount')\n",
    "top_answerers = answerers_table.sort_values(by='AnswerCount', ascending=False).head(3)\n",
    "\n",
    "top_tags = Tags[['TagName', 'Count']].sort_values(by='Count', ascending=False).head(3)\n",
    "\n",
    "print(\"Top 3 users with the most answers:\")\n",
    "print(top_answerers)\n",
    "\n",
    "print(\"\\nTop 3 most used tags:\")\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1 : First attach corresponding tags for answers by table join with questions table using Parent Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Qualified Answerers: 1160\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(answers_df, questions_df, left_on='ParentId', right_on='Id', suffixes=('_answer', '_question'))\n",
    "\n",
    "filtered_answers_df = merged_df[['Id_answer', 'OwnerUserId', 'Tags','ParentId']] \n",
    "filtered_answers_df = filtered_answers_df.drop_duplicates(['OwnerUserId','ParentId'])\n",
    "answerer_counts = filtered_answers_df.groupby('OwnerUserId').size()\n",
    "qualified_answerers = answerer_counts[answerer_counts >= 20].index\n",
    "print(\"No. of Qualified Answerers:\", len(qualified_answerers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step2 : Filter the answers using the qualified answers ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered answerers:    Id_answer  OwnerUserId                      Tags  ParentId\n",
      "0          3         11.0  |comments|anti-patterns|         1\n",
      "2         13          4.0  |comments|anti-patterns|         1\n",
      "3         56         17.0  |comments|anti-patterns|         1\n",
      "6        482        148.0  |comments|anti-patterns|         1\n",
      "8       1680        552.0  |comments|anti-patterns|         1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_answers = filtered_answers_df[filtered_answers_df['OwnerUserId'].isin(qualified_answerers)]\n",
    "print(\"filtered answerers:\" , filtered_answers.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step3 :  Filter tags and expand the answers tables by expanding rows for each tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualified Tags: 974\n"
     ]
    }
   ],
   "source": [
    "qualified_tags = Tags[Tags['Count'] >= 20]['Id']\n",
    "print(\"Qualified Tags:\", len(qualified_tags))\n",
    "\n",
    "tag_dict = Tags.set_index('TagName')['Id'].to_dict()\n",
    "\n",
    "tags_expanded = filtered_answers.copy()\n",
    "\n",
    "tags_expanded['Tags'] = tags_expanded['Tags'].str.split('|').apply(lambda x: x[1:-1])\n",
    "tags_expanded = tags_expanded.explode('Tags')\n",
    "tags_expanded['Tags'] = tags_expanded['Tags'].map(tag_dict)\n",
    "tags_expanded = tags_expanded[tags_expanded['Tags'].isin(qualified_tags)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step4 : Create Utility matrix from the filtered answers table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert Matrix: Tags         1.0     3.0     4.0     7.0     8.0     9.0     11.0    12.0    \\\n",
      "OwnerUserId                                                                   \n",
      "4.0            13.0     NaN     6.0     6.0    61.0    55.0     8.0     3.0   \n",
      "6.0             NaN     NaN     8.0     NaN     6.0     4.0     1.0     2.0   \n",
      "11.0            1.0     NaN     1.0     NaN     NaN     1.0     NaN     1.0   \n",
      "14.0            NaN     NaN     1.0     NaN     1.0     1.0     NaN     1.0   \n",
      "15.0            1.0     NaN     2.0     1.0     4.0     4.0     1.0     1.0   \n",
      "...             ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "356695.0        NaN     NaN     NaN     NaN     NaN     1.0     NaN     NaN   \n",
      "366014.0        NaN     NaN     NaN     NaN     NaN     NaN     1.0     NaN   \n",
      "373864.0        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "378329.0        1.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "379622.0        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "\n",
      "Tags         13.0    14.0    ...  4639.0  4646.0  4661.0  4682.0  4683.0  \\\n",
      "OwnerUserId                  ...                                           \n",
      "4.0             NaN     NaN  ...     NaN     NaN     NaN     2.0     1.0   \n",
      "6.0             NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "11.0            NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "14.0            NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "15.0            NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "...             ...     ...  ...     ...     ...     ...     ...     ...   \n",
      "356695.0        NaN     NaN  ...     NaN     NaN     1.0     NaN     NaN   \n",
      "366014.0        NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "373864.0        NaN     NaN  ...     NaN     NaN     2.0     NaN     NaN   \n",
      "378329.0        NaN     NaN  ...     NaN     NaN     1.0     NaN     NaN   \n",
      "379622.0        NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "\n",
      "Tags         4687.0  4690.0  4704.0  4720.0  4750.0  \n",
      "OwnerUserId                                          \n",
      "4.0             1.0     NaN     NaN     1.0     NaN  \n",
      "6.0             NaN     NaN     NaN     NaN     NaN  \n",
      "11.0            NaN     NaN     NaN     NaN     NaN  \n",
      "14.0            NaN     NaN     NaN     NaN     NaN  \n",
      "15.0            NaN     NaN     NaN     NaN     NaN  \n",
      "...             ...     ...     ...     ...     ...  \n",
      "356695.0        NaN     NaN     NaN     NaN     NaN  \n",
      "366014.0        NaN     NaN     NaN     NaN     NaN  \n",
      "373864.0        NaN     1.0     NaN     NaN     NaN  \n",
      "378329.0        NaN     NaN     NaN     5.0     1.0  \n",
      "379622.0        NaN     NaN     NaN     NaN     NaN  \n",
      "\n",
      "[1160 rows x 973 columns]\n",
      "Dimensions of the Expert matrix: (1160, 973)\n"
     ]
    }
   ],
   "source": [
    "expert_matrix = pd.pivot_table(\n",
    "    tags_expanded, \n",
    "    index='OwnerUserId', \n",
    "    columns='Tags', \n",
    "    aggfunc='size', \n",
    "    fill_value=np.nan\n",
    ")\n",
    "\n",
    "all_qualified_tags = pd.Series(qualified_tags, name='Tags')\n",
    "print(\"Expert Matrix:\", expert_matrix)\n",
    "print(\"Dimensions of the Expert matrix:\", expert_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step5: convert to numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13. nan  6. ... nan  1. nan]\n",
      " [nan nan  8. ... nan nan nan]\n",
      " [ 1. nan  1. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 1. nan nan ... nan  5.  1.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "expert_matrix = expert_matrix.to_numpy()\n",
    "print(expert_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Normalize the utility matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "normalize = lambda x: np.nan if np.isnan(x) else (np.floor(x / 3) if x < 15 else 5)\n",
    "vectorized_modify_entries = np.vectorize(normalize)\n",
    "utility_matrix = vectorized_modify_entries(expert_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4. nan  2. ... nan  0. nan]\n",
      " [nan nan  2. ... nan nan nan]\n",
      " [ 0. nan  0. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 0. nan nan ... nan  1.  0.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(utility_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility Matrix Metrics:\n",
      "Summation value of the utility matrix: 41180.0\n",
      "Highest row sum of the utility matrix: 1162.0\n",
      "Highest column sum of the utility matrix: 1403.0\n"
     ]
    }
   ],
   "source": [
    "sum_utility_matrix = np.nansum(utility_matrix)\n",
    "highest_row_sum = np.max(np.nansum(utility_matrix, axis=1))\n",
    "highest_column_sum = np.max(np.nansum(utility_matrix, axis=0))\n",
    "\n",
    "print(\"Utility Matrix Metrics:\")\n",
    "print(\"Summation value of the utility matrix:\", sum_utility_matrix)\n",
    "print(\"Highest row sum of the utility matrix:\", highest_row_sum)\n",
    "print(\"Highest column sum of the utility matrix:\", highest_column_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Create Train and Test matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test Matrix Metrics:\n",
      "Summation value of the train matrix: 40538.0\n",
      "Dimension of Test Matrix: (174, 146)\n",
      "Summation value of the test matrix: 642.0\n"
     ]
    }
   ],
   "source": [
    "num_users, num_tags = utility_matrix.shape\n",
    "user_cutoff = int(num_users * 0.85)\n",
    "tag_cutoff = int(num_tags * 0.85)\n",
    "\n",
    "train_matrix = utility_matrix.copy()\n",
    "train_matrix[user_cutoff:, tag_cutoff:] = np.nan\n",
    "train_matrix_sum = np.nansum(train_matrix)\n",
    "\n",
    "test_matrix = utility_matrix[user_cutoff:, tag_cutoff:]\n",
    "sum_test_matrix = np.nansum(test_matrix)\n",
    "\n",
    "print(\"Train and Test Matrix Metrics:\")\n",
    "print(\"Summation value of the train matrix:\", train_matrix_sum)\n",
    "print(\"Dimension of Test Matrix:\",test_matrix.shape)\n",
    "print(\"Summation value of the test matrix:\", sum_test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag-Tag Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-1: Calculate Similarity matrix for Tag-Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = utility_matrix[:user_cutoff]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_centered = df.sub(df.mean(axis=1), axis=0)\n",
    "\n",
    "similarity_matrix = df_centered.corr(method='pearson')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-2: Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, item_id, ratings, similarity_matrix, N , type):\n",
    "    \n",
    "    item_similarities = np.array(similarity_matrix[item_id][:tag_cutoff])\n",
    "    user_ratings = np.array(ratings[user_id])\n",
    "    similar_items = np.argsort(item_similarities)[::-1]\n",
    "\n",
    "    start = 0\n",
    "    while start < len(similar_items):\n",
    "        if np.isnan(item_similarities[similar_items[start]]): start += 1\n",
    "        else : break\n",
    "    \n",
    "    non_nan_similar_items = similar_items[start:]\n",
    "\n",
    "    rated_items = [item for item in non_nan_similar_items \n",
    "                         if not np.isnan(ratings[user_id][item])]\n",
    "    top_n_rated_items = rated_items[:N]  \n",
    "\n",
    "    if len(top_n_rated_items) < N:\n",
    "        return np.nanmean(user_ratings)\n",
    "    \n",
    "    top_n_ratings = user_ratings[top_n_rated_items]\n",
    "    top_n_similarities = item_similarities[top_n_rated_items]\n",
    "    \n",
    "    if type == \"A\":\n",
    "     return np.sum(top_n_ratings) / N\n",
    "\n",
    "    weighted_ratings_sum = np.dot(top_n_ratings, top_n_similarities)\n",
    "\n",
    "    similarity_sum = np.sum(top_n_similarities)\n",
    "    if similarity_sum != 0 : \n",
    "        predicted_rating = weighted_ratings_sum / similarity_sum\n",
    "    else : \n",
    "        predicted_rating = 0\n",
    "    \n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-3: Predict and Calculate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Average Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Average\n",
      "Loss with k=  2 : 0.8368331915377201\n",
      "Loss with k=  3 : 0.8068537836476033\n",
      "Loss with k=  5 : 0.7667057566908195\n"
     ]
    }
   ],
   "source": [
    "print(\"Simple Average\")\n",
    "for k in {2,3,5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x in range(user_cutoff,num_users):\n",
    "        for i in range(tag_cutoff,num_tags):\n",
    "            if np.isnan(utility_matrix[x][i]): continue\n",
    "            else : \n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x,i,utility_matrix,similarity_matrix,k,\"A\")\n",
    "                if(np.isnan(ans)) : \n",
    "                    print(x,i,'Hello')\n",
    "                loss = loss + (true_value - ans)*(true_value - ans)\n",
    "                cnt += 1\n",
    "    print(\"Loss with k =\",k,\":\",np.sqrt(loss / cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weighted Average Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average\n",
      "Loss with k=  2 : 0.8368997316823331\n",
      "Loss with k=  3 : 0.8066557194070604\n",
      "Loss with k=  5 : 0.768163144485662\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted Average\")\n",
    "for k in {2,3,5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x in range(user_cutoff,num_users):\n",
    "        for i in range(tag_cutoff,num_tags):\n",
    "            if np.isnan(utility_matrix[x][i]): continue\n",
    "            else : \n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x,i,utility_matrix,similarity_matrix,k,\"B\")\n",
    "                if(np.isnan(ans)) : \n",
    "                    print(x,i,'Hello')\n",
    "                loss = loss + (true_value - ans)*(true_value - ans)\n",
    "                cnt += 1\n",
    "    print(\"Loss with k =\",k,\":\",np.sqrt(loss / cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-User Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1160, 827)\n",
      "(1160, 1160)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = utility_matrix[:, :tag_cutoff]\n",
    "print(data.shape)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df_centered = df.sub(df.mean(axis=0), axis=1)\n",
    "\n",
    "similarity_matrix = df_centered.T.corr(method='pearson')\n",
    "print(similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, item_id, ratings, similarity_matrix, N, type, user_cutoff):\n",
    "    # Get the similarity scores for the target user with all other users within the user_cutoff\n",
    "    user_similarities = np.array(similarity_matrix[user_id][:user_cutoff])\n",
    "\n",
    "    # Get the ratings for the item by all users within the user_cutoff\n",
    "    item_ratings = np.array(ratings[:,item_id])\n",
    "\n",
    "    # Sort the similarities and take the top N most similar users (excluding the target user itself)\n",
    "    similar_users = np.argsort(user_similarities)[::-1]  # Sort indices by similarity in descending order\n",
    "\n",
    "    # Exclude the target user from the similar users list (if present)\n",
    "    similar_users = similar_users[similar_users != user_id]\n",
    "\n",
    "    start = 0\n",
    "    while start < len(similar_users):\n",
    "        if np.isnan(user_similarities[similar_users[start]]):\n",
    "            start += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    non_nan_similar_users = similar_users[start:]\n",
    "\n",
    "\n",
    "    # Filter out users who haven't rated the item\n",
    "    rated_users = [user for user in non_nan_similar_users if not np.isnan(ratings[user, item_id])]\n",
    "\n",
    "    top_n_rated_users = rated_users[:N]  # Select top N similar users\n",
    "\n",
    "    if len(top_n_rated_users) < N:\n",
    "        print(\"(tag_id:\",item_id,\")\",\"Less similar users for N: \",N,\" \")\n",
    "        return np.nanmean(item_ratings)\n",
    "\n",
    "    # Get the ratings from the top N similar users and the corresponding similarities\n",
    "    top_n_ratings = item_ratings[top_n_rated_users]\n",
    "    top_n_similarities = user_similarities[top_n_rated_users]\n",
    "\n",
    "    if type == \"A\":\n",
    "        return np.sum(top_n_ratings) / N\n",
    "\n",
    "    # Calculate the weighted sum of the ratings\n",
    "    weighted_ratings_sum = np.dot(top_n_ratings, top_n_similarities)\n",
    "\n",
    "    # Calculate the sum of the absolute values of the similarities\n",
    "    similarity_sum = np.sum(top_n_similarities)\n",
    "\n",
    "    # Return the weighted average as the predicted rating\n",
    "    if similarity_sum != 0:\n",
    "        predicted_rating = weighted_ratings_sum / similarity_sum\n",
    "    else:\n",
    "        print(\"1\")\n",
    "        predicted_rating = 0\n",
    "\n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with k = 2: 0.7006938793917289,2243\n",
      "Loss with k = 3: 0.690393644492957,2243\n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "Loss with k = 5: 0.6769631382163931,2243\n"
     ]
    }
   ],
   "source": [
    "for k in {2, 3, 5}:  # Different values of N (number of similar users to consider)\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for i in range(tag_cutoff, num_tags):  # Iterate through items\n",
    "     for x in range(user_cutoff, num_users):  # Iterate through users\n",
    "            if np.isnan(utility_matrix[x][i]):  # Skip if the user hasn't rated the item\n",
    "                continue\n",
    "            else:\n",
    "                true_value = utility_matrix[x][i]  # Actual rating\n",
    "                ans = predict_rating(x, i, utility_matrix, similarity_matrix, k, \"A\", user_cutoff)  # Predict rating\n",
    "                \n",
    "                if np.isnan(ans): \n",
    "                    print(x, i, 'Hello')\n",
    "                    continue  # Skip NaN predictions to avoid errors\n",
    "                \n",
    "                # Calculate squared error\n",
    "                loss += (true_value - ans) ** 2\n",
    "                cnt += 1\n",
    "    \n",
    "    # Calculate and print RMSE (Root Mean Squared Error) for the current value of k\n",
    "    print(f\"Loss with k = {k}: {np.sqrt(loss / cnt)},{cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with k = 2: 1.0257074115306568\n",
      "Loss with k = 3: 0.7457370435666525\n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "(tag_id: 861 ) Less similar users for N:  5  \n",
      "Loss with k = 5: 0.6830546662035217\n"
     ]
    }
   ],
   "source": [
    "for k in {2, 3, 5}:  # Different values of N (number of similar users to consider)\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for i in range(tag_cutoff, num_tags):  # Iterate through items\n",
    "     for x in range(user_cutoff, num_users):  # Iterate through users\n",
    "            if np.isnan(utility_matrix[x][i]):  # Skip if the user hasn't rated the item\n",
    "                continue\n",
    "            else:\n",
    "                true_value = utility_matrix[x][i]  # Actual rating\n",
    "                ans = predict_rating(x, i, utility_matrix, similarity_matrix, k, \"B\", user_cutoff)  # Predict rating\n",
    "                \n",
    "                if np.isnan(ans): \n",
    "                    print(x, i, 'Hello')\n",
    "                    continue  # Skip NaN predictions to avoid errors\n",
    "                \n",
    "                # Calculate squared error\n",
    "                loss += (true_value - ans) ** 2\n",
    "                cnt += 1\n",
    "    \n",
    "    # Calculate and print RMSE (Root Mean Squared Error) for the current value of k\n",
    "    print(f\"Loss with k = {k}: {np.sqrt(loss / cnt)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### Question 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class MatrixFactorizationSGD:\n",
    "    def __init__(self, R, K, alpha=0.0005, epochs=10, lambda1=0, lambda2=0):\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K  # Number of latent factors\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.epochs = epochs  # Number of full passes (epochs)\n",
    "        self.lambda1 = lambda1  # Regularization parameter for P\n",
    "        self.lambda2 = lambda2  # Regularization parameter for Q\n",
    "        self.P = np.random.normal(scale=1./K, size=(self.num_users, K))\n",
    "        self.Q = np.random.normal(scale=1./K, size=(self.num_items, K))\n",
    "        # self.P = np.random.rand(self.num_users, K)  # User-feature matrix\n",
    "        # self.Q = np.random.rand(self.num_items, K)  # Item-feature matrix\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the matrix factorization model using stochastic gradient descent (SGD) over multiple epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(self.num_users):\n",
    "                for j in range(self.num_items):\n",
    "                    if not np.isnan(self.R[i][j]):  # Skip NaN values\n",
    "                        # Compute the prediction error\n",
    "                        eij = self.R[i][j] - np.dot(self.P[i, :], self.Q[j, :])\n",
    "\n",
    "                        # Update P and Q matrices with regularization terms lambda1 and lambda2\n",
    "                        for k in range(self.K):\n",
    "                            self.P[i][k] += self.alpha * (2 * eij * self.Q[j][k] - 2 * self.lambda1 * self.P[i][k])\n",
    "                            self.Q[j][k] += self.alpha * (2 * eij * self.P[i][k] - 2 * self.lambda2 * self.Q[j][k])\n",
    "            \n",
    "            # Compute and print the loss at the end of each epoch\n",
    "            loss = self.compute_loss()\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}: Loss = {loss}\")\n",
    "        \n",
    "        return self.P, self.Q\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \"\"\"\n",
    "        Compute the cost function (loss) with two regularization parameters.\n",
    "        The loss includes the squared error and regularization terms.\n",
    "        \"\"\"\n",
    "        predicted_R = np.dot(self.P, self.Q.T)\n",
    "        loss = 0\n",
    "        num_non_nan_entries = 0\n",
    "\n",
    "        for i in range(self.num_users):\n",
    "            for j in range(self.num_items):\n",
    "                if not np.isnan(self.R[i][j]):  # Only consider non-NaN values\n",
    "                    error = self.R[i][j] - predicted_R[i][j]\n",
    "                    loss += error ** 2\n",
    "                    num_non_nan_entries += 1\n",
    "        \n",
    "        # Adding regularization terms for P and Q matrices\n",
    "        loss += self.lambda1 * np.sum(self.P ** 2)  # Regularization for P\n",
    "        loss += self.lambda2 * np.sum(self.Q ** 2)  # Regularization for Q\n",
    "\n",
    "        # Normalize the loss by the number of non-NaN entries\n",
    "        loss /= num_non_nan_entries\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict(self, test_matrix):\n",
    "        \"\"\"\n",
    "        Predict the rating matrix for the test set.\n",
    "        \"\"\"\n",
    "        predicted_R = np.dot(self.P, self.Q.T)\n",
    "        predicted_R_test = np.copy(test_matrix)\n",
    "        \n",
    "        # Only fill the predictions where test_matrix is not NaN\n",
    "        for i in range(test_matrix.shape[0]):\n",
    "            for j in range(test_matrix.shape[1]):\n",
    "                if test_matrix[i][j] is not None:\n",
    "                    predicted_R_test[i][j] = predicted_R[i][j]\n",
    "\n",
    "        return predicted_R_test\n",
    "\n",
    "    def rmse(self, predicted_R, test_matrix):\n",
    "        \"\"\"\n",
    "        Compute the Root Mean Square Error (RMSE) between the predicted and actual ratings in the test matrix.\n",
    "        \"\"\"\n",
    "        mask = ~np.isnan(test_matrix)\n",
    "        actual_ratings = test_matrix[mask]\n",
    "        predicted_ratings = predicted_R[mask]\n",
    "        return np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "\n",
    "# Test the updated modular code with train and test matrices\n",
    "def run_tests():\n",
    "    # Assume train_matrix and test_matrix are predefined\n",
    "    R_train = train_matrix\n",
    "    R_test = test_matrix\n",
    "\n",
    "    # Parameters\n",
    "    latent_factors = [2, 5, 10]\n",
    "    alpha = 0.0005\n",
    "    epochs = 25\n",
    "    lambda1List = [0, 0.001, 0.05, 0.5]  # Regularization parameter for P\n",
    "    lambda2List = [0, 0.003, 0.05, 0.75]  # Regularization parameter for Q\n",
    "\n",
    "    for K in latent_factors:\n",
    "        for lambda1, lambda2 in zip(lambda1List, lambda2List):\n",
    "            mf_reg = MatrixFactorizationSGD(R_train, K, alpha, epochs, lambda1=lambda1, lambda2=lambda2)\n",
    "            P_reg, Q_reg = mf_reg.train()\n",
    "\n",
    "            # Predict for the training matrix\n",
    "            predicted_R_train = np.dot(P_reg, Q_reg.T)\n",
    "            train_rmse = mf_reg.rmse(predicted_R_train, R_train)\n",
    "\n",
    "            # Predict for the test matrix\n",
    "            predicted_R_reg = mf_reg.predict(R_test)\n",
    "            rmse_reg = mf_reg.rmse(predicted_R_reg, R_test)\n",
    "\n",
    "            print(f\"Latent Factors: {K}\")\n",
    "            print(f\"RMSE on train data with regularization (lambda1={lambda1}, lambda2={lambda2}): {train_rmse}\")\n",
    "            print(f\"RMSE on test data with regularization (lambda1={lambda1}, lambda2={lambda2}): {rmse_reg}\")\n",
    "\n",
    "# Example usage:\n",
    "run_tests()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
