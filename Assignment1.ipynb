{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment1: Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "posts_df = pd.read_csv('./csv/Posts.csv')\n",
    "Tags = pd.read_csv('./csv/Tags.csv')\n",
    "\n",
    "answers_df = posts_df[posts_df['PostTypeId'] == 2][['Id', 'OwnerUserId', 'ParentId']]\n",
    "answers_df = answers_df.drop_duplicates(subset=['OwnerUserId','ParentId'])\n",
    "answers_df['ParentId'] = answers_df['ParentId'].astype(int)\n",
    "\n",
    "questions_df = posts_df[posts_df['PostTypeId'] == 1][['Id', 'Tags']]\n",
    "\n",
    "questions_df['Id'] = questions_df['Id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 users with the most answers:\n",
      "       OwnerUserId  AnswerCount\n",
      "3189        9113.0         2838\n",
      "19912     177980.0         2318\n",
      "557         1204.0         2042\n",
      "\n",
      "Top 3 most used tags:\n",
      "    TagName  Count\n",
      "259  design   5162\n",
      "114      c#   4931\n",
      "37     java   4929\n"
     ]
    }
   ],
   "source": [
    "answerers_table = answers_df.groupby('OwnerUserId').size().reset_index(name='AnswerCount')\n",
    "top_answerers = answerers_table.sort_values(by='AnswerCount', ascending=False).head(3)\n",
    "\n",
    "top_tags = Tags[['TagName', 'Count']].sort_values(by='Count', ascending=False).head(3)\n",
    "\n",
    "print(\"Top 3 users with the most answers:\")\n",
    "print(top_answerers)\n",
    "\n",
    "print(\"\\nTop 3 most used tags:\")\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1 : First attach corresponding tags for answers by table join with questions table using Parent Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Qualified Answerers: 1160\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(answers_df, questions_df, left_on='ParentId', right_on='Id', suffixes=('_answer', '_question'))\n",
    "\n",
    "filtered_answers_df = merged_df[['Id_answer', 'OwnerUserId', 'Tags','ParentId']] \n",
    "filtered_answers_df = filtered_answers_df.drop_duplicates(['OwnerUserId','ParentId'])\n",
    "answerer_counts = filtered_answers_df.groupby('OwnerUserId').size()\n",
    "qualified_answerers = answerer_counts[answerer_counts >= 20].index\n",
    "print(\"No. of Qualified Answerers:\", len(qualified_answerers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step2 : Filter the answers using the qualified answers ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered answerers:    Id_answer  OwnerUserId                                               Tags  \\\n",
      "0          3         11.0                           |comments|anti-patterns|   \n",
      "3         13          4.0                           |comments|anti-patterns|   \n",
      "4         20          6.0                     |productivity|time-management|   \n",
      "6         23         11.0                     |productivity|time-management|   \n",
      "8         26         17.0  |business|project-management|development-process|   \n",
      "\n",
      "   ParentId  \n",
      "0         1  \n",
      "3         1  \n",
      "4         9  \n",
      "6         9  \n",
      "8         4  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_answers = filtered_answers_df[filtered_answers_df['OwnerUserId'].isin(qualified_answerers)]\n",
    "print(\"filtered answerers:\" , filtered_answers.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step3 :  Filter tags and expand the answers tables by expanding rows for each tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualified Tags: 974\n"
     ]
    }
   ],
   "source": [
    "qualified_tags = Tags[Tags['Count'] >= 20]['Id']\n",
    "print(\"Qualified Tags:\", len(qualified_tags))\n",
    "\n",
    "tag_dict = Tags.set_index('TagName')['Id'].to_dict()\n",
    "\n",
    "tags_expanded = filtered_answers.copy()\n",
    "\n",
    "tags_expanded['Tags'] = tags_expanded['Tags'].str.split('|').apply(lambda x: x[1:-1])\n",
    "tags_expanded = tags_expanded.explode('Tags')\n",
    "tags_expanded['Tags'] = tags_expanded['Tags'].map(tag_dict)\n",
    "tags_expanded = tags_expanded[tags_expanded['Tags'].isin(qualified_tags)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step4 : Create Utility matrix from the filtered answers table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert Matrix: Tags         1     3     4     7     8     9     11    12    13    14    ...  \\\n",
      "OwnerUserId                                                              ...   \n",
      "4            13.0   NaN   6.0   6.0  61.0  55.0   8.0   3.0   NaN   NaN  ...   \n",
      "6             NaN   NaN   8.0   NaN   6.0   4.0   1.0   2.0   NaN   NaN  ...   \n",
      "11            1.0   NaN   1.0   NaN   NaN   1.0   NaN   1.0   NaN   NaN  ...   \n",
      "14            NaN   NaN   1.0   NaN   1.0   1.0   NaN   1.0   NaN   NaN  ...   \n",
      "15            1.0   NaN   2.0   1.0   4.0   4.0   1.0   1.0   NaN   NaN  ...   \n",
      "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
      "356695        NaN   NaN   NaN   NaN   NaN   1.0   NaN   NaN   NaN   NaN  ...   \n",
      "366014        NaN   NaN   NaN   NaN   NaN   NaN   1.0   NaN   NaN   NaN  ...   \n",
      "373864        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "378329        1.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "379622        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "\n",
      "Tags         4639  4646  4661  4682  4683  4687  4690  4704  4720  4750  \n",
      "OwnerUserId                                                              \n",
      "4             NaN   NaN   NaN   2.0   1.0   1.0   NaN   NaN   1.0   NaN  \n",
      "6             NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "11            NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "14            NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "15            NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "356695        NaN   NaN   1.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "366014        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "373864        NaN   NaN   2.0   NaN   NaN   NaN   1.0   NaN   NaN   NaN  \n",
      "378329        NaN   NaN   1.0   NaN   NaN   NaN   NaN   NaN   5.0   1.0  \n",
      "379622        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "\n",
      "[1160 rows x 973 columns]\n",
      "Dimensions of the Expert matrix: (1160, 973)\n"
     ]
    }
   ],
   "source": [
    "expert_matrix_df = pd.pivot_table(\n",
    "    tags_expanded, \n",
    "    index='OwnerUserId', \n",
    "    columns='Tags', \n",
    "    aggfunc='size', \n",
    "    fill_value=np.nan\n",
    ")\n",
    "\n",
    "expert_matrix_df.index = expert_matrix_df.index.astype(int)\n",
    "expert_matrix_df.columns = expert_matrix_df.columns.astype(int)\n",
    "\n",
    "all_qualified_tags = pd.Series(qualified_tags, name='Tags')\n",
    "print(\"Expert Matrix:\", expert_matrix_df)\n",
    "print(\"Dimensions of the Expert matrix:\", expert_matrix_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step5: convert to numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13. nan  6. ... nan  1. nan]\n",
      " [nan nan  8. ... nan nan nan]\n",
      " [ 1. nan  1. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 1. nan nan ... nan  5.  1.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "expert_matrix = expert_matrix_df.to_numpy()\n",
    "print(expert_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Normalize the utility matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65537/1149839958.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  utility_matrix_df = expert_matrix_df.applymap(normalize)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "normalize = lambda x: np.nan if np.isnan(x) else (np.floor(x / 3) if x < 15 else 5)\n",
    "vectorized_modify_entries = np.vectorize(normalize)\n",
    "utility_matrix = vectorized_modify_entries(expert_matrix)\n",
    "utility_matrix_df = expert_matrix_df.applymap(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4. nan  2. ... nan  0. nan]\n",
      " [nan nan  2. ... nan nan nan]\n",
      " [ 0. nan  0. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 0. nan nan ... nan  1.  0.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(utility_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility Matrix Metrics:\n",
      "Summation value of the utility matrix: 41180.0\n",
      "Highest row sum of the utility matrix: 1162.0\n",
      "Highest column sum of the utility matrix: 1403.0\n"
     ]
    }
   ],
   "source": [
    "sum_utility_matrix = np.nansum(utility_matrix)\n",
    "highest_row_sum = np.max(np.nansum(utility_matrix, axis=1))\n",
    "highest_column_sum = np.max(np.nansum(utility_matrix, axis=0))\n",
    "\n",
    "print(\"Utility Matrix Metrics:\")\n",
    "print(\"Summation value of the utility matrix:\", sum_utility_matrix)\n",
    "print(\"Highest row sum of the utility matrix:\", highest_row_sum)\n",
    "print(\"Highest column sum of the utility matrix:\", highest_column_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Create Train and Test matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test Matrix Metrics:\n",
      "Summation value of the train matrix: 40538.0\n",
      "Dimension of Test Matrix: (174, 146)\n",
      "Summation value of the test matrix: 642.0\n"
     ]
    }
   ],
   "source": [
    "num_users, num_tags = utility_matrix.shape\n",
    "user_cutoff = int(num_users * 0.85)\n",
    "tag_cutoff = int(num_tags * 0.85)\n",
    "\n",
    "train_matrix = utility_matrix.copy()\n",
    "train_df = utility_matrix_df.copy()\n",
    "\n",
    "\n",
    "train_matrix[user_cutoff:, tag_cutoff:] = np.nan\n",
    "train_df.iloc[user_cutoff: ,tag_cutoff:] = np.nan\n",
    "train_matrix_sum = np.nansum(train_matrix)\n",
    "\n",
    "test_matrix = utility_matrix[user_cutoff:, tag_cutoff:]\n",
    "test_df = utility_matrix_df.iloc[user_cutoff: , tag_cutoff:]\n",
    "sum_test_matrix = np.nansum(test_matrix)\n",
    "\n",
    "print(\"Train and Test Matrix Metrics:\")\n",
    "print(\"Summation value of the train matrix:\", train_matrix_sum)\n",
    "print(\"Dimension of Test Matrix:\",test_matrix.shape)\n",
    "print(\"Summation value of the test matrix:\", sum_test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag-Tag Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-1: Calculate Similarity matrix for Tag-Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = utility_matrix[:user_cutoff]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_centered = df.sub(df.mean(axis=1), axis=0)\n",
    "\n",
    "similarity_matrix = df_centered.corr(method='pearson')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-2: Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, item_id, ratings, similarity_matrix, N , type):\n",
    "    \n",
    "    item_similarities = np.array(similarity_matrix[item_id][:tag_cutoff])\n",
    "    user_ratings = np.array(ratings[user_id])\n",
    "    similar_items = np.argsort(item_similarities)[::-1]\n",
    "\n",
    "    start = 0\n",
    "    while start < len(similar_items):\n",
    "        if np.isnan(item_similarities[similar_items[start]]): start += 1\n",
    "        else : break\n",
    "    \n",
    "    non_nan_similar_items = similar_items[start:]\n",
    "\n",
    "    rated_items = [item for item in non_nan_similar_items \n",
    "                         if not np.isnan(ratings[user_id][item])]\n",
    "    top_n_rated_items = rated_items[:N]  \n",
    "\n",
    "    if len(top_n_rated_items) < N:\n",
    "        return np.nanmean(user_ratings)\n",
    "    \n",
    "    top_n_ratings = user_ratings[top_n_rated_items]\n",
    "    top_n_similarities = item_similarities[top_n_rated_items]\n",
    "    \n",
    "    if type == \"A\":\n",
    "     return np.sum(top_n_ratings) / N\n",
    "\n",
    "    weighted_ratings_sum = np.dot(top_n_ratings, top_n_similarities)\n",
    "\n",
    "    similarity_sum = np.sum(top_n_similarities)\n",
    "    if similarity_sum != 0 : \n",
    "        predicted_rating = weighted_ratings_sum / similarity_sum\n",
    "    else : \n",
    "        predicted_rating = 0\n",
    "    \n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-3: Predict and Calculate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Average Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Average\n",
      "Loss with k = 2 : 0.8368331915377201\n",
      "Loss with k = 3 : 0.8068537836476033\n",
      "Loss with k = 5 : 0.7667057566908195\n"
     ]
    }
   ],
   "source": [
    "print(\"Simple Average\")\n",
    "for k in {2,3,5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x in range(user_cutoff,num_users):\n",
    "        for i in range(tag_cutoff,num_tags):\n",
    "            if np.isnan(utility_matrix[x][i]): continue\n",
    "            else : \n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x,i,utility_matrix,similarity_matrix,k,\"A\")\n",
    "                if(np.isnan(ans)) : \n",
    "                    print(x,i,'Hello')\n",
    "                loss = loss + (true_value - ans)*(true_value - ans)\n",
    "                cnt += 1\n",
    "    print(\"Loss with k =\",k,\":\",np.sqrt(loss / cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weighted Average Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average\n",
      "Loss with k = 2 : 0.8368997316823331\n",
      "Loss with k = 3 : 0.8066557194070604\n",
      "Loss with k = 5 : 0.768163144485662\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted Average\")\n",
    "for k in {2,3,5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x in range(user_cutoff,num_users):\n",
    "        for i in range(tag_cutoff,num_tags):\n",
    "            if np.isnan(utility_matrix[x][i]): continue\n",
    "            else : \n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x,i,utility_matrix,similarity_matrix,k,\"B\")\n",
    "                if(np.isnan(ans)) : \n",
    "                    print(x,i,'Hello')\n",
    "                loss = loss + (true_value - ans)*(true_value - ans)\n",
    "                cnt += 1\n",
    "    print(\"Loss with k =\",k,\":\",np.sqrt(loss / cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-User Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = utility_matrix[:, :tag_cutoff]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df_centered = df.sub(df.mean(axis=0), axis=1)\n",
    "\n",
    "similarity_matrix = df_centered.T.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Function with Simple Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, item_id, ratings, similarity_matrix, N, type, user_cutoff):\n",
    "    user_similarities = np.array(similarity_matrix[user_id][:user_cutoff])\n",
    "    item_ratings = np.array(ratings[:,item_id])\n",
    "    similar_users = np.argsort(user_similarities)[::-1]\n",
    "    similar_users = similar_users[similar_users != user_id]\n",
    "\n",
    "    start = 0\n",
    "    while start < len(similar_users):\n",
    "        if np.isnan(user_similarities[similar_users[start]]):\n",
    "            start += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    non_nan_similar_users = similar_users[start:]\n",
    "\n",
    "    rated_users = [user for user in non_nan_similar_users if not np.isnan(ratings[user, item_id])]\n",
    "\n",
    "    top_n_rated_users = rated_users[:N]\n",
    "\n",
    "    if len(top_n_rated_users) < N:\n",
    "        return np.nanmean(item_ratings)\n",
    "\n",
    "    top_n_ratings = item_ratings[top_n_rated_users]\n",
    "    top_n_similarities = user_similarities[top_n_rated_users]\n",
    "\n",
    "    if type == \"A\":\n",
    "        return np.sum(top_n_ratings)/N\n",
    "\n",
    "    weighted_ratings_sum = np.dot(top_n_ratings, top_n_similarities)\n",
    "\n",
    "    similarity_sum = np.sum(top_n_similarities)\n",
    "\n",
    "    if similarity_sum != 0:\n",
    "        predicted_rating = weighted_ratings_sum / similarity_sum\n",
    "    else:\n",
    "        predicted_rating = 0\n",
    "\n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Average\n",
      "Loss with k = 2: 0.7006938793917289,2243\n",
      "Loss with k = 3: 0.690393644492957,2243\n",
      "Loss with k = 5: 0.6769631382163931,2243\n"
     ]
    }
   ],
   "source": [
    "print(\"Simple Average\")\n",
    "for k in {2, 3, 5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for i in range(tag_cutoff, num_tags):\n",
    "     for x in range(user_cutoff, num_users):\n",
    "            if np.isnan(utility_matrix[x][i]):\n",
    "                continue\n",
    "            else:\n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x, i, utility_matrix, similarity_matrix, k, \"A\", user_cutoff)\n",
    "                \n",
    "                if np.isnan(ans): \n",
    "                    print(x, i, 'Hello')\n",
    "                    continue\n",
    "                \n",
    "                loss += (true_value - ans) ** 2\n",
    "                cnt += 1\n",
    "    \n",
    "    print(f\"Loss with k = {k}: {np.sqrt(loss / cnt)},{cnt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average\n",
      "Loss with k = 2: 1.0257074115306568\n",
      "Loss with k = 3: 0.7457370435666525\n",
      "Loss with k = 5: 0.6830546662035217\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted Average\")\n",
    "for k in {2, 3, 5}: \n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for i in range(tag_cutoff, num_tags):\n",
    "     for x in range(user_cutoff, num_users):  \n",
    "            if np.isnan(utility_matrix[x][i]):\n",
    "                continue\n",
    "            else:\n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x, i, utility_matrix, similarity_matrix, k, \"B\", user_cutoff)\n",
    "                \n",
    "                if np.isnan(ans): \n",
    "                    continue\n",
    "                \n",
    "                loss += (true_value - ans) ** 2\n",
    "                cnt += 1\n",
    "\n",
    "    print(f\"Loss with k = {k}: {np.sqrt(loss / cnt)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### Question 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(42)\n",
    "\n",
    "class MatrixFactorizationSGD:\n",
    "    def __init__(self, R, K, alpha=0.0005, epochs=10, lambda1=0, lambda2=0):\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K \n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.P = np.random.rand(self.num_users, K)  \n",
    "        self.Q = np.random.rand(self.num_items, K)  \n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(self.num_users):\n",
    "                for j in range(self.num_items):\n",
    "                    if not np.isnan(self.R[i][j]):\n",
    "                        eij = self.R[i][j] - np.dot(self.P[i, :], self.Q[j, :])\n",
    "                        self.P[i][:] += self.alpha * (2 * eij * self.Q[j][:] - 2 * self.lambda1 * self.P[i][:])\n",
    "                        self.Q[j][:] += self.alpha * (2 * eij * self.P[i][:] - 2 * self.lambda2 * self.Q[j][:])\n",
    "            \n",
    "            loss = self.compute_loss()\n",
    "            if epoch%10 == 9:\n",
    "                print(f\"Epoch {epoch + 1}/{self.epochs}: Loss = {loss}\")\n",
    "        \n",
    "        return self.P, self.Q\n",
    "\n",
    "    def compute_loss(self):\n",
    "        predicted_R = np.dot(self.P, self.Q.T)\n",
    "        loss = 0\n",
    "        num_non_nan_entries = 0\n",
    "\n",
    "        for i in range(self.num_users):\n",
    "            for j in range(self.num_items):\n",
    "                if not np.isnan(self.R[i][j]):\n",
    "                    error = self.R[i][j] - predicted_R[i][j]\n",
    "                    loss += error ** 2\n",
    "                    num_non_nan_entries += 1\n",
    "        \n",
    "        loss += self.lambda1 * np.sum(self.P ** 2)\n",
    "        loss += self.lambda2 * np.sum(self.Q ** 2)\n",
    "        loss /= num_non_nan_entries\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict(self, test_matrix):\n",
    "        predicted_R = np.dot(self.P, self.Q.T)\n",
    "        predicted_R_test = np.copy(test_matrix)\n",
    "        \n",
    "        for i in range(test_matrix.shape[0]):\n",
    "            for j in range(test_matrix.shape[1]):\n",
    "                if test_matrix[i][j] is not None:\n",
    "                    predicted_R_test[i][j] = predicted_R[i+user_cutoff][j+tag_cutoff]\n",
    "\n",
    "        return predicted_R_test\n",
    "    \n",
    "    def rmse(self, predicted_R, matrix):\n",
    "        mask = ~np.isnan(matrix)\n",
    "        actual_ratings = matrix[mask]\n",
    "        predicted_ratings = predicted_R[mask]\n",
    "        return np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent Factor: 2\n",
      "Epoch 10/50: Loss = 0.5885543155884819\n",
      "Epoch 20/50: Loss = 0.4597366962724134\n",
      "Epoch 30/50: Loss = 0.4168497550149226\n",
      "Epoch 40/50: Loss = 0.40426059447369866\n",
      "Epoch 50/50: Loss = 0.39991378082190576\n",
      "Latent Factor: 2\n",
      "Epoch 10/50: Loss = 0.5757072904263831\n",
      "Epoch 20/50: Loss = 0.4534093759199884\n",
      "Epoch 30/50: Loss = 0.41431094209033714\n",
      "Epoch 40/50: Loss = 0.40286220423009544\n",
      "Epoch 50/50: Loss = 0.39868735534986627\n",
      "Latent Factor: 2\n",
      "Epoch 10/50: Loss = 0.6010154467025955\n",
      "Epoch 20/50: Loss = 0.4751747494818502\n",
      "Epoch 30/50: Loss = 0.42803216518832077\n",
      "Epoch 40/50: Loss = 0.41253194758841366\n",
      "Epoch 50/50: Loss = 0.4067904283081891\n",
      "Latent Factor: 2\n",
      "Epoch 10/50: Loss = 0.8463731150416814\n",
      "Epoch 20/50: Loss = 0.8670694002551558\n",
      "Epoch 30/50: Loss = 0.8797561203120179\n",
      "Epoch 40/50: Loss = 0.8884117825427251\n",
      "Epoch 50/50: Loss = 0.8947539432540357\n",
      "Latent Factor: 5\n",
      "Epoch 10/50: Loss = 0.755757963856201\n",
      "Epoch 20/50: Loss = 0.5578551785793089\n",
      "Epoch 30/50: Loss = 0.44458483354302275\n",
      "Epoch 40/50: Loss = 0.4083968803242825\n",
      "Epoch 50/50: Loss = 0.39467340826658853\n",
      "Latent Factor: 5\n",
      "Epoch 10/50: Loss = 0.764344860973094\n",
      "Epoch 20/50: Loss = 0.5838337101887034\n",
      "Epoch 30/50: Loss = 0.4649929006557257\n",
      "Epoch 40/50: Loss = 0.4126006358802988\n",
      "Epoch 50/50: Loss = 0.3883050524195493\n",
      "Latent Factor: 5\n",
      "Epoch 10/50: Loss = 0.7627922328543403\n",
      "Epoch 20/50: Loss = 0.5874781403134516\n",
      "Epoch 30/50: Loss = 0.4710233453809116\n",
      "Epoch 40/50: Loss = 0.4238951815392775\n",
      "Epoch 50/50: Loss = 0.4060464846908087\n",
      "Latent Factor: 5\n",
      "Epoch 10/50: Loss = 0.8658095142182541\n",
      "Epoch 20/50: Loss = 0.8726033318450628\n",
      "Epoch 30/50: Loss = 0.881248322940081\n",
      "Epoch 40/50: Loss = 0.8880935496761825\n",
      "Epoch 50/50: Loss = 0.8935570424429228\n",
      "Latent Factor: 10\n",
      "Epoch 10/50: Loss = 0.907806015702202\n",
      "Epoch 20/50: Loss = 0.5674347603408997\n",
      "Epoch 30/50: Loss = 0.43665126981407615\n",
      "Epoch 40/50: Loss = 0.40266904498587036\n",
      "Epoch 50/50: Loss = 0.3807206939077011\n",
      "Latent Factor: 10\n",
      "Epoch 10/50: Loss = 0.9089061654244738\n",
      "Epoch 20/50: Loss = 0.574176630994363\n",
      "Epoch 30/50: Loss = 0.4334674663166917\n",
      "Epoch 40/50: Loss = 0.39569241087056445\n",
      "Epoch 50/50: Loss = 0.3727068567992656\n",
      "Latent Factor: 10\n",
      "Epoch 10/50: Loss = 0.9191369903120066\n",
      "Epoch 20/50: Loss = 0.7219231681843418\n",
      "Epoch 30/50: Loss = 0.5019304819420379\n",
      "Epoch 40/50: Loss = 0.4216057667535573\n",
      "Epoch 50/50: Loss = 0.39766746703233896\n",
      "Latent Factor: 10\n",
      "Epoch 10/50: Loss = 0.9041043053907173\n",
      "Epoch 20/50: Loss = 0.8857949482485427\n",
      "Epoch 30/50: Loss = 0.8877523581221527\n",
      "Epoch 40/50: Loss = 0.891536628562527\n",
      "Epoch 50/50: Loss = 0.8952936513679942\n"
     ]
    }
   ],
   "source": [
    "R_train = train_matrix\n",
    "R_test = test_matrix\n",
    "\n",
    "latent_factors = [2, 5, 10]\n",
    "alpha = 0.0005\n",
    "epochs = 50\n",
    "lambda1List = [0, 0.001, 0.05, 0.5]\n",
    "lambda2List = [0, 0.003, 0.05, 0.75]\n",
    "\n",
    "train_rmse_results = []\n",
    "test_rmse_results = []\n",
    "\n",
    "for K in latent_factors:\n",
    "    train_rmse_k = []\n",
    "    test_rmse_k = []\n",
    "    for lambda1, lambda2 in zip(lambda1List, lambda2List):\n",
    "        mf_reg = MatrixFactorizationSGD(R_train, K, alpha, epochs, lambda1=lambda1, lambda2=lambda2)\n",
    "        print(f\"Latent Factor: {K}\")\n",
    "        P_reg, Q_reg = mf_reg.train()\n",
    "        \n",
    "        predicted_R_train = np.dot(P_reg, Q_reg.T)\n",
    "        train_rmse = mf_reg.rmse(predicted_R_train, R_train)\n",
    "\n",
    "        predicted_R_reg = mf_reg.predict(R_test)\n",
    "        test_rmse = mf_reg.rmse(predicted_R_reg, R_test)\n",
    "\n",
    "        train_rmse_k.append(train_rmse)\n",
    "        test_rmse_k.append(test_rmse)\n",
    "\n",
    "        \n",
    "    train_rmse_results.append(train_rmse_k)\n",
    "    test_rmse_results.append(test_rmse_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent Factor  Lambda1   Lambda2   Train RMSE     Test RMSE      \n",
      "2              0         0         0.632387       0.689827       \n",
      "2              0.001     0.003     0.631408       0.696004       \n",
      "2              0.05      0.05      0.637591       0.700942       \n",
      "2              0.5       0.75      0.945666       0.830768       \n",
      "\n",
      "5              0         0         0.628230       0.727873       \n",
      "5              0.001     0.003     0.623127       0.744690       \n",
      "5              0.05      0.05      0.636860       0.730593       \n",
      "5              0.5       0.75      0.944772       0.828121       \n",
      "\n",
      "10             0         0         0.617026       0.916975       \n",
      "10             0.001     0.003     0.610465       0.908043       \n",
      "10             0.05      0.05      0.629923       0.852763       \n",
      "10             0.5       0.75      0.945302       0.830399       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Latent Factor':<15}{'Lambda1':<10}{'Lambda2':<10}{'Train RMSE':<15}{'Test RMSE':<15}\")\n",
    "\n",
    "count = 0\n",
    "for lf in latent_factors:\n",
    "    for l1, l2, train_rmse, test_rmse in zip(lambda1List, lambda2List, train_rmse_results[count], test_rmse_results[count]):\n",
    "        print(f\"{lf:<15}{l1:<10}{l2:<10}{train_rmse:<15.6f}{test_rmse:<15.6f}\")\n",
    "    print()\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a): Similarity techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, KNNBaseline\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_surprise_df(train_df,test_df):\n",
    "    surprise_train_df = train_df.stack().reset_index()\n",
    "    surprise_train_df.columns = ['user', 'item', 'rating']\n",
    "\n",
    "    surprise_test_df = test_df.stack().reset_index()\n",
    "    surprise_test_df.columns = ['user', 'item', 'rating']\n",
    "\n",
    "    return surprise_train_df[pd.notna(surprise_train_df['rating'])] , surprise_test_df[pd.notna(surprise_test_df['rating'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise_train_df,surprise_test_df = format_surprise_df(train_df,test_df)\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "train_data = Dataset.load_from_df(surprise_train_df, reader)\n",
    "trainset = train_data.build_full_trainset()\n",
    "testset = list(surprise_test_df.itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: tag - tag Suprise Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "rmse_results_tag_tag = {}\n",
    "\n",
    "for N in {2,3,5}:\n",
    "    algo = KNNBaseline(k=N, sim_options={'name': 'pearson_baseline', 'user_based': False})\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    rmse = accuracy.rmse(predictions, verbose=False)\n",
    "    rmse_results_tag_tag[N] = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surprise Library RMSE Results (Tag-Tag):\n",
      "N=2: RMSE=0.7669281403985956\n",
      "N=3: RMSE=0.7275695777976227\n",
      "N=5: RMSE=0.694168988415908\n"
     ]
    }
   ],
   "source": [
    "print(\"Surprise Library RMSE Results (Tag-Tag):\")\n",
    "for N in {2,3,5}:\n",
    "    print(f\"N={N}: RMSE={rmse_results_tag_tag[N]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: user - user Suprise Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "rmse_results_user_user = {}\n",
    "\n",
    "for N in {2,3,5}:\n",
    "    algo = KNNBaseline(k=N, sim_options={'name': 'pearson_baseline', 'user_based': True})\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    rmse = accuracy.rmse(predictions, verbose=False)\n",
    "    rmse_results_tag_tag[N] = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surprise Library RMSE Results (User-User):\n",
      "N=2: RMSE=0.671089179871824\n",
      "N=3: RMSE=0.643941897960725\n",
      "N=5: RMSE=0.6277610480570359\n"
     ]
    }
   ],
   "source": [
    "print(\"Surprise Library RMSE Results (User-User):\")\n",
    "for N in {2,3,5}:\n",
    "    print(f\"N={N}: RMSE={rmse_results_tag_tag[N]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b): SVD algorithm using Suprise Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import  SVD\n",
    "\n",
    "lambda1List = [0, 0.001, 0.05, 0.5]\n",
    "lambda2List = [0, 0.003, 0.05, 0.75]\n",
    "\n",
    "test_rmse_results = []\n",
    "\n",
    "for K in latent_factors:\n",
    "    test_rmse_k = []\n",
    "    for lambda1, lambda2 in zip(lambda1List, lambda2List):\n",
    "         model = SVD(n_factors=N, lr_all=0.0005, reg_bi=0, reg_bu=0, reg_pu=lambda1, reg_qi=lambda2)\n",
    "         model.fit(trainset)\n",
    "         predictions = model.test(testset)\n",
    "         rmse = accuracy.rmse(predictions, verbose=False)\n",
    "         test_rmse_k.append(rmse)\n",
    "    test_rmse_results.append(test_rmse_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent Factor  Lambda1   Lambda2   Test RMSE      \n",
      "2              0         0         0.748609       \n",
      "2              0.001     0.003     0.747660       \n",
      "2              0.05      0.05      0.748339       \n",
      "2              0.5       0.75      0.747960       \n",
      "\n",
      "5              0         0         0.748061       \n",
      "5              0.001     0.003     0.748024       \n",
      "5              0.05      0.05      0.747836       \n",
      "5              0.5       0.75      0.747878       \n",
      "\n",
      "10             0         0         0.748472       \n",
      "10             0.001     0.003     0.748162       \n",
      "10             0.05      0.05      0.747852       \n",
      "10             0.5       0.75      0.747920       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Latent Factor':<15}{'Lambda1':<10}{'Lambda2':<10}{'Test RMSE':<15}\")\n",
    "\n",
    "\n",
    "count = 0\n",
    "for lf in latent_factors:\n",
    "    for l1, l2,  test_rmse in zip(lambda1List, lambda2List, test_rmse_results[count]):\n",
    "        print(f\"{lf:<15}{l1:<10}{l2:<10}{test_rmse:<15.6f}\")\n",
    "    print()\n",
    "    count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
