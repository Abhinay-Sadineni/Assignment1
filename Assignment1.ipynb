{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment1: Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "posts_df = pd.read_csv('./csv/Posts.csv')\n",
    "Tags = pd.read_csv('./csv/Tags.csv')\n",
    "\n",
    "answers_df = posts_df[posts_df['PostTypeId'] == 2][['Id', 'OwnerUserId', 'ParentId']]\n",
    "answers_df = answers_df.drop_duplicates(subset=['OwnerUserId','ParentId'])\n",
    "answers_df['ParentId'] = answers_df['ParentId'].astype(int)\n",
    "\n",
    "questions_df = posts_df[posts_df['PostTypeId'] == 1][['Id', 'Tags']]\n",
    "\n",
    "questions_df['Id'] = questions_df['Id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 users with the most answers:\n",
      "       OwnerUserId  AnswerCount\n",
      "3189        9113.0         2838\n",
      "19912     177980.0         2318\n",
      "557         1204.0         2042\n",
      "\n",
      "Top 3 most used tags:\n",
      "    TagName  Count\n",
      "259  design   5162\n",
      "114      c#   4931\n",
      "37     java   4929\n"
     ]
    }
   ],
   "source": [
    "answerers_table = answers_df.groupby('OwnerUserId').size().reset_index(name='AnswerCount')\n",
    "top_answerers = answerers_table.sort_values(by='AnswerCount', ascending=False).head(3)\n",
    "\n",
    "top_tags = Tags[['TagName', 'Count']].sort_values(by='Count', ascending=False).head(3)\n",
    "\n",
    "print(\"Top 3 users with the most answers:\")\n",
    "print(top_answerers)\n",
    "\n",
    "print(\"\\nTop 3 most used tags:\")\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1 : First attach corresponding tags for answers by table join with questions table using Parent Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Qualified Answerers: 1160\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(answers_df, questions_df, left_on='ParentId', right_on='Id', suffixes=('_answer', '_question'))\n",
    "\n",
    "filtered_answers_df = merged_df[['Id_answer', 'OwnerUserId', 'Tags','ParentId']] \n",
    "filtered_answers_df = filtered_answers_df.drop_duplicates(['OwnerUserId','ParentId'])\n",
    "answerer_counts = filtered_answers_df.groupby('OwnerUserId').size()\n",
    "qualified_answerers = answerer_counts[answerer_counts >= 20].index\n",
    "print(\"No. of Qualified Answerers:\", len(qualified_answerers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step2 : Filter the answers using the qualified answers ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered answerers:    Id_answer  OwnerUserId                      Tags  ParentId\n",
      "0          3         11.0  |comments|anti-patterns|         1\n",
      "2         13          4.0  |comments|anti-patterns|         1\n",
      "3         56         17.0  |comments|anti-patterns|         1\n",
      "6        482        148.0  |comments|anti-patterns|         1\n",
      "8       1680        552.0  |comments|anti-patterns|         1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_answers = filtered_answers_df[filtered_answers_df['OwnerUserId'].isin(qualified_answerers)]\n",
    "print(\"filtered answerers:\" , filtered_answers.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step3 :  Filter tags and expand the answers tables by expanding rows for each tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualified Tags: 974\n"
     ]
    }
   ],
   "source": [
    "qualified_tags = Tags[Tags['Count'] >= 20]['Id']\n",
    "print(\"Qualified Tags:\", len(qualified_tags))\n",
    "\n",
    "tag_dict = Tags.set_index('TagName')['Id'].to_dict()\n",
    "\n",
    "tags_expanded = filtered_answers.copy()\n",
    "\n",
    "tags_expanded['Tags'] = tags_expanded['Tags'].str.split('|').apply(lambda x: x[1:-1])\n",
    "tags_expanded = tags_expanded.explode('Tags')\n",
    "tags_expanded['Tags'] = tags_expanded['Tags'].map(tag_dict)\n",
    "tags_expanded = tags_expanded[tags_expanded['Tags'].isin(qualified_tags)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step4 : Create Utility matrix from the filtered answers table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert Matrix: Tags         1.0     3.0     4.0     7.0     8.0     9.0     11.0    12.0    \\\n",
      "OwnerUserId                                                                   \n",
      "4.0            13.0     NaN     6.0     6.0    61.0    55.0     8.0     3.0   \n",
      "6.0             NaN     NaN     8.0     NaN     6.0     4.0     1.0     2.0   \n",
      "11.0            1.0     NaN     1.0     NaN     NaN     1.0     NaN     1.0   \n",
      "14.0            NaN     NaN     1.0     NaN     1.0     1.0     NaN     1.0   \n",
      "15.0            1.0     NaN     2.0     1.0     4.0     4.0     1.0     1.0   \n",
      "...             ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "356695.0        NaN     NaN     NaN     NaN     NaN     1.0     NaN     NaN   \n",
      "366014.0        NaN     NaN     NaN     NaN     NaN     NaN     1.0     NaN   \n",
      "373864.0        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "378329.0        1.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "379622.0        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "\n",
      "Tags         13.0    14.0    ...  4639.0  4646.0  4661.0  4682.0  4683.0  \\\n",
      "OwnerUserId                  ...                                           \n",
      "4.0             NaN     NaN  ...     NaN     NaN     NaN     2.0     1.0   \n",
      "6.0             NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "11.0            NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "14.0            NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "15.0            NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "...             ...     ...  ...     ...     ...     ...     ...     ...   \n",
      "356695.0        NaN     NaN  ...     NaN     NaN     1.0     NaN     NaN   \n",
      "366014.0        NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "373864.0        NaN     NaN  ...     NaN     NaN     2.0     NaN     NaN   \n",
      "378329.0        NaN     NaN  ...     NaN     NaN     1.0     NaN     NaN   \n",
      "379622.0        NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN   \n",
      "\n",
      "Tags         4687.0  4690.0  4704.0  4720.0  4750.0  \n",
      "OwnerUserId                                          \n",
      "4.0             1.0     NaN     NaN     1.0     NaN  \n",
      "6.0             NaN     NaN     NaN     NaN     NaN  \n",
      "11.0            NaN     NaN     NaN     NaN     NaN  \n",
      "14.0            NaN     NaN     NaN     NaN     NaN  \n",
      "15.0            NaN     NaN     NaN     NaN     NaN  \n",
      "...             ...     ...     ...     ...     ...  \n",
      "356695.0        NaN     NaN     NaN     NaN     NaN  \n",
      "366014.0        NaN     NaN     NaN     NaN     NaN  \n",
      "373864.0        NaN     1.0     NaN     NaN     NaN  \n",
      "378329.0        NaN     NaN     NaN     5.0     1.0  \n",
      "379622.0        NaN     NaN     NaN     NaN     NaN  \n",
      "\n",
      "[1160 rows x 973 columns]\n",
      "Dimensions of the Expert matrix: (1160, 973)\n"
     ]
    }
   ],
   "source": [
    "expert_matrix = pd.pivot_table(\n",
    "    tags_expanded, \n",
    "    index='OwnerUserId', \n",
    "    columns='Tags', \n",
    "    aggfunc='size', \n",
    "    fill_value=np.nan\n",
    ")\n",
    "\n",
    "all_qualified_tags = pd.Series(qualified_tags, name='Tags')\n",
    "print(\"Expert Matrix:\", expert_matrix)\n",
    "print(\"Dimensions of the Expert matrix:\", expert_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "#### Step5: convert to numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13. nan  6. ... nan  1. nan]\n",
      " [nan nan  8. ... nan nan nan]\n",
      " [ 1. nan  1. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 1. nan nan ... nan  5.  1.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "expert_matrix = expert_matrix.to_numpy()\n",
    "print(expert_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Normalize the utility matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "normalize = lambda x: np.nan if np.isnan(x) else (np.floor(x / 3) if x < 15 else 5)\n",
    "vectorized_modify_entries = np.vectorize(normalize)\n",
    "utility_matrix = vectorized_modify_entries(expert_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4. nan  2. ... nan  0. nan]\n",
      " [nan nan  2. ... nan nan nan]\n",
      " [ 0. nan  0. ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [ 0. nan nan ... nan  1.  0.]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(utility_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility Matrix Metrics:\n",
      "Summation value of the utility matrix: 41180.0\n",
      "Highest row sum of the utility matrix: 1162.0\n",
      "Highest column sum of the utility matrix: 1403.0\n"
     ]
    }
   ],
   "source": [
    "sum_utility_matrix = np.nansum(utility_matrix)\n",
    "highest_row_sum = np.max(np.nansum(utility_matrix, axis=1))\n",
    "highest_column_sum = np.max(np.nansum(utility_matrix, axis=0))\n",
    "\n",
    "print(\"Utility Matrix Metrics:\")\n",
    "print(\"Summation value of the utility matrix:\", sum_utility_matrix)\n",
    "print(\"Highest row sum of the utility matrix:\", highest_row_sum)\n",
    "print(\"Highest column sum of the utility matrix:\", highest_column_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Create Train and Test matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test Matrix Metrics:\n",
      "Summation value of the train matrix: 40538.0\n",
      "Dimension of Test Matrix: (174, 146)\n",
      "Summation value of the test matrix: 642.0\n"
     ]
    }
   ],
   "source": [
    "num_users, num_tags = utility_matrix.shape\n",
    "user_cutoff = int(num_users * 0.85)\n",
    "tag_cutoff = int(num_tags * 0.85)\n",
    "\n",
    "train_matrix = utility_matrix.copy()\n",
    "train_matrix[user_cutoff:, tag_cutoff:] = np.nan\n",
    "train_matrix_sum = np.nansum(train_matrix)\n",
    "\n",
    "test_matrix = utility_matrix[user_cutoff:, tag_cutoff:]\n",
    "sum_test_matrix = np.nansum(test_matrix)\n",
    "\n",
    "print(\"Train and Test Matrix Metrics:\")\n",
    "print(\"Summation value of the train matrix:\", train_matrix_sum)\n",
    "print(\"Dimension of Test Matrix:\",test_matrix.shape)\n",
    "print(\"Summation value of the test matrix:\", sum_test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag-Tag Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-1: Calculate Similarity matrix for Tag-Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = utility_matrix[:user_cutoff]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_centered = df.sub(df.mean(axis=1), axis=0)\n",
    "\n",
    "similarity_matrix = df_centered.corr(method='pearson')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-2: Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, item_id, ratings, similarity_matrix, N , type):\n",
    "    \n",
    "    item_similarities = np.array(similarity_matrix[item_id][:tag_cutoff])\n",
    "    user_ratings = np.array(ratings[user_id])\n",
    "    similar_items = np.argsort(item_similarities)[::-1]\n",
    "\n",
    "    start = 0\n",
    "    while start < len(similar_items):\n",
    "        if np.isnan(item_similarities[similar_items[start]]): start += 1\n",
    "        else : break\n",
    "    \n",
    "    non_nan_similar_items = similar_items[start:]\n",
    "\n",
    "    rated_items = [item for item in non_nan_similar_items \n",
    "                         if not np.isnan(ratings[user_id][item])]\n",
    "    top_n_rated_items = rated_items[:N]  \n",
    "\n",
    "    if len(top_n_rated_items) < N:\n",
    "        return np.nanmean(user_ratings)\n",
    "    \n",
    "    top_n_ratings = user_ratings[top_n_rated_items]\n",
    "    top_n_similarities = item_similarities[top_n_rated_items]\n",
    "    \n",
    "    if type == \"A\":\n",
    "     return np.sum(top_n_ratings) / N\n",
    "\n",
    "    weighted_ratings_sum = np.dot(top_n_ratings, top_n_similarities)\n",
    "\n",
    "    similarity_sum = np.sum(top_n_similarities)\n",
    "    if similarity_sum != 0 : \n",
    "        predicted_rating = weighted_ratings_sum / similarity_sum\n",
    "    else : \n",
    "        predicted_rating = 0\n",
    "    \n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-3: Predict and Calculate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Average Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Average\n",
      "Loss with k = 2 : 0.8368331915377201\n",
      "Loss with k = 3 : 0.8068537836476033\n",
      "Loss with k = 5 : 0.7667057566908195\n"
     ]
    }
   ],
   "source": [
    "print(\"Simple Average\")\n",
    "for k in {2,3,5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x in range(user_cutoff,num_users):\n",
    "        for i in range(tag_cutoff,num_tags):\n",
    "            if np.isnan(utility_matrix[x][i]): continue\n",
    "            else : \n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x,i,utility_matrix,similarity_matrix,k,\"A\")\n",
    "                if(np.isnan(ans)) : \n",
    "                    print(x,i,'Hello')\n",
    "                loss = loss + (true_value - ans)*(true_value - ans)\n",
    "                cnt += 1\n",
    "    print(\"Loss with k =\",k,\":\",np.sqrt(loss / cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weighted Average Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average\n",
      "Loss with k = 2 : 0.8368997316823331\n",
      "Loss with k = 3 : 0.8066557194070604\n",
      "Loss with k = 5 : 0.768163144485662\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted Average\")\n",
    "for k in {2,3,5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x in range(user_cutoff,num_users):\n",
    "        for i in range(tag_cutoff,num_tags):\n",
    "            if np.isnan(utility_matrix[x][i]): continue\n",
    "            else : \n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x,i,utility_matrix,similarity_matrix,k,\"B\")\n",
    "                if(np.isnan(ans)) : \n",
    "                    print(x,i,'Hello')\n",
    "                loss = loss + (true_value - ans)*(true_value - ans)\n",
    "                cnt += 1\n",
    "    print(\"Loss with k =\",k,\":\",np.sqrt(loss / cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-User Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = utility_matrix[:, :tag_cutoff]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df_centered = df.sub(df.mean(axis=0), axis=1)\n",
    "\n",
    "similarity_matrix = df_centered.T.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Function with Simple Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, item_id, ratings, similarity_matrix, N, type, user_cutoff):\n",
    "    user_similarities = np.array(similarity_matrix[user_id][:user_cutoff])\n",
    "    item_ratings = np.array(ratings[:,item_id])\n",
    "    similar_users = np.argsort(user_similarities)[::-1]\n",
    "    similar_users = similar_users[similar_users != user_id]\n",
    "\n",
    "    start = 0\n",
    "    while start < len(similar_users):\n",
    "        if np.isnan(user_similarities[similar_users[start]]):\n",
    "            start += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    non_nan_similar_users = similar_users[start:]\n",
    "\n",
    "    rated_users = [user for user in non_nan_similar_users if not np.isnan(ratings[user, item_id])]\n",
    "\n",
    "    top_n_rated_users = rated_users[:N]\n",
    "\n",
    "    if len(top_n_rated_users) < N:\n",
    "        return np.nanmean(item_ratings)\n",
    "\n",
    "    top_n_ratings = item_ratings[top_n_rated_users]\n",
    "    top_n_similarities = user_similarities[top_n_rated_users]\n",
    "\n",
    "    if type == \"A\":\n",
    "        return np.sum(top_n_ratings)/N\n",
    "\n",
    "    weighted_ratings_sum = np.dot(top_n_ratings, top_n_similarities)\n",
    "\n",
    "    similarity_sum = np.sum(top_n_similarities)\n",
    "\n",
    "    if similarity_sum != 0:\n",
    "        predicted_rating = weighted_ratings_sum / similarity_sum\n",
    "    else:\n",
    "        predicted_rating = 0\n",
    "\n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Average\n",
      "Loss with k = 2: 0.7006938793917289,2243\n",
      "Loss with k = 3: 0.690393644492957,2243\n",
      "Loss with k = 5: 0.6769631382163931,2243\n"
     ]
    }
   ],
   "source": [
    "print(\"Simple Average\")\n",
    "for k in {2, 3, 5}:\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for i in range(tag_cutoff, num_tags):\n",
    "     for x in range(user_cutoff, num_users):\n",
    "            if np.isnan(utility_matrix[x][i]):\n",
    "                continue\n",
    "            else:\n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x, i, utility_matrix, similarity_matrix, k, \"A\", user_cutoff)\n",
    "                \n",
    "                if np.isnan(ans): \n",
    "                    print(x, i, 'Hello')\n",
    "                    continue\n",
    "                \n",
    "                loss += (true_value - ans) ** 2\n",
    "                cnt += 1\n",
    "    \n",
    "    print(f\"Loss with k = {k}: {np.sqrt(loss / cnt)},{cnt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average\n",
      "Loss with k = 2: 1.0257074115306568\n",
      "Loss with k = 3: 0.7457370435666525\n",
      "Loss with k = 5: 0.6830546662035217\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted Average\")\n",
    "for k in {2, 3, 5}: \n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for i in range(tag_cutoff, num_tags):\n",
    "     for x in range(user_cutoff, num_users):  \n",
    "            if np.isnan(utility_matrix[x][i]):\n",
    "                continue\n",
    "            else:\n",
    "                true_value = utility_matrix[x][i]\n",
    "                ans = predict_rating(x, i, utility_matrix, similarity_matrix, k, \"B\", user_cutoff)\n",
    "                \n",
    "                if np.isnan(ans): \n",
    "                    continue\n",
    "                \n",
    "                loss += (true_value - ans) ** 2\n",
    "                cnt += 1\n",
    "\n",
    "    print(f\"Loss with k = {k}: {np.sqrt(loss / cnt)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "### Question 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class MatrixFactorizationSGD:\n",
    "    def __init__(self, R, K, alpha=0.0005, epochs=10, lambda1=0, lambda2=0):\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K \n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.P = np.random.normal(scale=1./K, size=(self.num_users, K))\n",
    "        self.Q = np.random.normal(scale=1./K, size=(self.num_items, K))\n",
    "        # self.P = np.random.rand(self.num_users, K)  # User-feature matrix\n",
    "        # self.Q = np.random.rand(self.num_items, K)  # Item-feature matrix\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(self.num_users):\n",
    "                for j in range(self.num_items):\n",
    "                    if not np.isnan(self.R[i][j]):\n",
    "                        eij = self.R[i][j] - np.dot(self.P[i, :], self.Q[j, :])\n",
    "                        for k in range(self.K):\n",
    "                            self.P[i][k] += self.alpha * (2 * eij * self.Q[j][k] - 2 * self.lambda1 * self.P[i][k])\n",
    "                            self.Q[j][k] += self.alpha * (2 * eij * self.P[i][k] - 2 * self.lambda2 * self.Q[j][k])\n",
    "            \n",
    "            loss = self.compute_loss()\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}: Loss = {loss}\")\n",
    "        \n",
    "        return self.P, self.Q\n",
    "\n",
    "    def compute_loss(self):\n",
    "        predicted_R = np.dot(self.P, self.Q.T)\n",
    "        loss = 0\n",
    "        num_non_nan_entries = 0\n",
    "\n",
    "        for i in range(self.num_users):\n",
    "            for j in range(self.num_items):\n",
    "                if not np.isnan(self.R[i][j]):\n",
    "                    error = self.R[i][j] - predicted_R[i][j]\n",
    "                    loss += error ** 2\n",
    "                    num_non_nan_entries += 1\n",
    "        \n",
    "        loss += self.lambda1 * np.sum(self.P ** 2)\n",
    "        loss += self.lambda2 * np.sum(self.Q ** 2)\n",
    "        loss /= num_non_nan_entries\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict(self, test_matrix):\n",
    "        predicted_R = np.dot(self.P, self.Q.T)\n",
    "        predicted_R_test = np.copy(test_matrix)\n",
    "        \n",
    "        for i in range(test_matrix.shape[0]):\n",
    "            for j in range(test_matrix.shape[1]):\n",
    "                if test_matrix[i][j] is not None:\n",
    "                    predicted_R_test[i][j] = predicted_R[i][j]\n",
    "\n",
    "        return predicted_R_test\n",
    "    \n",
    "    def rmse(self, predicted_R, matrix):\n",
    "        mask = ~np.isnan(matrix)\n",
    "        actual_ratings = matrix[mask]\n",
    "        predicted_ratings = predicted_R[mask]\n",
    "        return np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lambda1, lambda2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lambda1List, lambda2List):\n\u001b[1;32m     12\u001b[0m     mf_reg \u001b[38;5;241m=\u001b[39m MatrixFactorizationSGD(R_train, K, alpha, epochs, lambda1\u001b[38;5;241m=\u001b[39mlambda1, lambda2\u001b[38;5;241m=\u001b[39mlambda2)\n\u001b[0;32m---> 13\u001b[0m     P_reg, Q_reg \u001b[38;5;241m=\u001b[39m \u001b[43mmf_reg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     predicted_R_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(P_reg, Q_reg\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m     16\u001b[0m     train_rmse \u001b[38;5;241m=\u001b[39m mf_reg\u001b[38;5;241m.\u001b[39mrmse(predicted_R_train, R_train)\n",
      "Cell \u001b[0;32mIn[128], line 28\u001b[0m, in \u001b[0;36mMatrixFactorizationSGD.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP[i][k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m eij \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ[j][k] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda1 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP[i][k])\n\u001b[1;32m     26\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ[j][k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m eij \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP[i][k] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda2 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ[j][k])\n\u001b[0;32m---> 28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ\n",
      "Cell \u001b[0;32mIn[128], line 40\u001b[0m, in \u001b[0;36mMatrixFactorizationSGD.compute_loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_users):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_items):\n\u001b[0;32m---> 40\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mR\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     41\u001b[0m             error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR[i][j] \u001b[38;5;241m-\u001b[39m predicted_R[i][j]\n\u001b[1;32m     42\u001b[0m             loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m error \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "R_train = train_matrix\n",
    "R_test = test_matrix\n",
    "\n",
    "latent_factors = [2, 5, 10]\n",
    "alpha = 0.0005\n",
    "epochs = 25\n",
    "lambda1List = [0, 0.001, 0.05, 0.5]\n",
    "lambda2List = [0, 0.003, 0.05, 0.75]\n",
    "\n",
    "for K in latent_factors:\n",
    "    for lambda1, lambda2 in zip(lambda1List, lambda2List):\n",
    "        mf_reg = MatrixFactorizationSGD(R_train, K, alpha, epochs, lambda1=lambda1, lambda2=lambda2)\n",
    "        P_reg, Q_reg = mf_reg.train()\n",
    "\n",
    "        predicted_R_train = np.dot(P_reg, Q_reg.T)\n",
    "        train_rmse = mf_reg.rmse(predicted_R_train, R_train)\n",
    "\n",
    "        predicted_R_reg = mf_reg.predict(R_test)\n",
    "        rmse_reg = mf_reg.rmse(predicted_R_reg, R_test)\n",
    "\n",
    "        print(f\"Latent Factor: {K}\")\n",
    "        print(f\"RMSE on train data with regularization (lambda1={lambda1}, lambda2={lambda2}): {train_rmse}\")\n",
    "        print(f\"RMSE on test data with regularization (lambda1={lambda1}, lambda2={lambda2}): {rmse_reg}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
